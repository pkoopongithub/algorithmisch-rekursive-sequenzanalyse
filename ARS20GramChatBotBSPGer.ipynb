{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Algorithmisch Rekursive Sequenzanalyse 2.0**  \n*Nutzen der optimierten Grammatik für einen Chatbot*  \nPaul Koop  \nNovember 2024  \npost@paul-koop.org",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Um ein erweitertes Python-Programm zu erstellen, das Open-Source-Modelle wie GPT-Neo oder GPT-J über Hugging Face nutzt, und mit einer spezifischen Grammatikstruktur arbeitet, wie du sie angegeben hast, gehen wir schrittweise vor. Das Programm wird Hugging Face's API verwenden, um die Modelle GPT-Neo oder GPT-J zu nutzen, und auf der Grundlage einer vorgegebenen Grammatik Anfragen generieren.\n\n### Voraussetzungen\n1. Installiere die Hugging Face `transformers`-Bibliothek:\n   ```bash\n   pip install transformers\n   ```\n\n2. Wenn du ein kostenloses Modell bei Hugging Face verwenden möchtest, benötigst du ein API-Schlüssel. Registriere dich bei [Hugging Face](https://huggingface.co/) und erstelle einen API-Schlüssel.\n\n3. Die Grammatik, die du bereitgestellt hast, legt fest, wie die Struktur von Konversationen aussieht. Wir werden die Grammatik implementieren und dann ein LLM-Modell verwenden, um die Antworten zu generieren.\n\n### Schritt 1: Grammatik definieren\nZunächst implementieren wir die Grammatik, die du bereitgestellt hast, und verwenden sie, um Konversationen zu generieren. Wir werden dies als eine Art \"Template\" für die Konversationslogik betrachten.\n\n### Schritt 2: API-Aufruf an Hugging Face\nWir werden die **GPT-Neo** oder **GPT-J** Modelle von Hugging Face verwenden, um auf die Anfragen basierend auf der Grammatik zu antworten.\n\nHier ist ein vollständiges Beispielprogramm in Python:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\n\n# Die Grammatik bleibt unverändert\ngrammar = { \n    '<Start>': [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>': [['KBG', 'VBG', 1.0]],\n    '<Bedarf>': [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>': [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>': [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>': [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n}\n\n# Die Antworten mit den angepassten Sätzen für K (Kunde) und V (Verkäufer)\nresponses = { \n    \"KBG\": \"Guten Tag, können Sie mir helfen?\",\n    \"VBG\": \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\": \"Ich brauche frische Kirschen.\",\n    \"VBBd\": \"Was genau benötigen Sie?\",\n    \"KBA\": \"Die sind mir zu teuer.\",\n    \"VBA\": \"Die sind preiswert.\",\n    \"KAA\": \"Zu dem Preis nehme ich die Ware.\",\n    \"VAA\": \"Zu dem Preis kann ich Ihnen die Ware anbieten.\",\n    \"KAV\": \"Auf Wiedersehen!\",\n    \"VAV\": \"Tschüss, bis zum nächsten Mal!\"\n}\n\n# Funktion zur Auswahl der nächsten Antwort basierend auf der Grammatik\ndef generate_response(step='<Start>', role='K'):\n    if step not in grammar:\n        return responses.get(step, \"[Keine Antwort verfügbar]\")\n    \n    # Auswahl eines Produktionspfads basierend auf Wahrscheinlichkeiten\n    production = random.choices(\n        grammar[step], \n        weights=[p[-1] for p in grammar[step]]\n    )[0]\n    \n    # Hole das nächste Symbol\n    response_chain = []\n    for symbol in production[:-1]:  # Exkludiert die Wahrscheinlichkeit am Ende\n        \n        if symbol in responses:\n            # Rolle berücksichtigen\n            if symbol.startswith(role):\n                response_chain.append(responses[symbol])\n            \n            # Rolle wechseln\n            role = 'V' if role == 'K' else 'K'\n            \n        else:\n            # Rekursive Verarbeitung, falls symbol nicht terminal ist\n            response_chain.append(generate_response(symbol, role))\n    \n    return \" \".join(response_chain)\n\n# Beispielhafte Nutzung\nprint(generate_response())\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "\n## Erläuterung des Programms\n\nDas Programm implementiert eine einfache dialogbasierte Interaktion zwischen einem Kunden (K) und einem Verkäufer (V), indem es eine vorgegebene Grammatik und entsprechende Antworten verwendet. Die Grammatikstruktur sowie die Rollenverteilung zwischen Kunde und Verkäufer steuern die Sequenz der Antworten.\n\n### Erklärung der Grammatiknutzung\n\n1. **Rollenwechsel**:\n   In der Funktion `generate_response` wird zwischen den Rollen `K` (Kunde) und `V` (Verkäufer) gewechselt. Wenn die aktuelle Rolle `K` ist, wählt das Programm eine Antwort für den Kunden und wechselt danach die Rolle zu `V`, sodass die nächste Antwort vom Verkäufer kommt. Dieser Wechsel ermöglicht, dass die Konversation einem realistischen Verkaufsgespräch ähnelt.\n\n2. **Produktion basierend auf Wahrscheinlichkeiten**:\n   Mithilfe der Funktion `random.choices()` wird ein Produktionspfad der Grammatik basierend auf den gegebenen Wahrscheinlichkeiten ausgewählt. Dadurch wird bei jeder Ausführung des Programms ein etwas anderer Dialogverlauf erzeugt, abhängig von der vorgegebenen Wahrscheinlichkeit jeder Produktionsregel. Das erlaubt eine gewisse Dynamik im Gesprächsfluss.\n\n3. **Rekursion für nicht-terminale Symbole**:\n   Wenn das aktuelle Symbol ein nicht-terminales Zeichen ist (z. B. `<BedarfSegment>`), ruft die Funktion `generate_response` sich selbst rekursiv auf, bis ein Terminalzeichen (eine tatsächliche Antwort) erreicht wird. Dieser rekursive Ansatz ermöglicht die Verarbeitung komplexer, mehrstufiger Dialoge und gewährleistet, dass die Sequenz von Kunden- und Verkäuferantworten den Regeln der Grammatik folgt.\n\n4. **Zusammenfügen der Antworten**:\n   Die Antworten werden in einer Liste namens `response_chain` gesammelt und am Ende als vollständige Konversationskette (String) zurückgegeben. Dieser Ansatz sorgt dafür, dass der gesamte Dialog nahtlos und korrekt formatiert zurückgegeben wird.\n\nMit diesem Ansatz passt das Programm die Antworten dynamisch an die Grammatikstruktur und die Rollen an und gibt einen konsistenten Dialog basierend auf der Grammatikstruktur aus.\n\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Der Ansatz, ein LLM mit einer empirisch optimierten Grammatik zu steuern, ist in gewisser Weise neu und innovativ. Während klassische Chatbots und regelbasierte Systeme häufig feste Dialogflüsse und definierte Entscheidungsbäume nutzen, setzen moderne LLM-basierte Chatbots auf flexible, kontextgesteuerte Antworten, die auf Wahrscheinlichkeitsverteilungen innerhalb des Modells basieren. Durch eine gezielt eingesetzte, optimierte Grammatik wird allerdings versucht, diese Flexibilität mit einer strukturierten Gesprächsführung zu kombinieren.\n\nIn den letzten Jahren gab es bereits verschiedene Ansätze, LLMs über regelbasierte Systeme oder grammatikähnliche Strukturen gezielt zu steuern. Diese Systeme wurden jedoch oft zur Inhaltseinschränkung oder für die Erzeugung spezialisierter, kontextbezogener Antworten genutzt. Eine empirisch optimierte Grammatik – basierend auf Daten realer Gespräche und speziell zur Beschreibung und Steuerung des Dialogflusses verwendet – kombiniert die Vorteile beider Ansätze:\n\n1. **Erhalt natürlicher Dialogdynamik**: Das LLM bringt die Fähigkeit ein, auf Nutzeranfragen flexibel zu reagieren, ohne in festgelegten Entscheidungsbäumen gefangen zu sein.\n  \n2. **Struktur und Steuerung**: Die Grammatik bringt eine zusätzliche Steuerungsebene ein, die bestimmte Gesprächsmuster priorisiert oder wahrscheinlicher macht. Damit kann der Dialogfluss in eine bestimmte Richtung gelenkt werden, z.B. basierend auf erprobten Interaktionen oder typischen Gesprächsstrategien (wie in Verkaufsgesprächen).\n\n3. **Flexibilität und Anpassung an spezifische Szenarien**: Durch die Optimierung der Wahrscheinlichkeiten können bestimmte Sequenzen und Antworten bevorzugt werden, was nützlich ist, um die Erwartungen an eine spezifische Gesprächsstruktur (wie in Verkaufsgesprächen) zu erfüllen, ohne die Variabilität eines LLM vollständig zu verlieren.\n\nInsgesamt ist der Ansatz, empirisch optimierte Grammatiken gezielt zur Steuerung eines LLM-basierten Dialogsystems einzusetzen, ein spannender Versuch, die Balance zwischen Flexibilität und Struktur zu erreichen und könnte besonders für domänenspezifische Anwendungen, wie Verkaufsgespräche, Beratungen oder Support-Interaktionen, vielversprechend sein.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\nimport openai\n\n# Definierte Grammatik für die Gesprächsstruktur\ngrammar = { \n    '<Start>': [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>': [['KBG', 'VBG', 1.0]],\n    '<Bedarf>': [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>': [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>': [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>': [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n}\n\n# Antworten für die Gesprächskomponenten\nresponses = { \n    \"KBG\": \"Guten Tag, können Sie mir helfen?\",\n    \"VBG\": \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\": \"Ich brauche frische Kirschen.\",\n    \"VBBd\": \"Was genau benötigen Sie?\",\n    \"KBA\": \"Die sind mir zu teuer.\",\n    \"VBA\": \"Die sind preiswert.\",\n    \"KAA\": \"Zu dem Preis nehme ich die Ware.\",\n    \"VAA\": \"Zu dem Preis kann ich Ihnen die Ware anbieten.\",\n    \"KAV\": \"Auf Wiedersehen!\",\n    \"VAV\": \"Tschüss, bis zum nächsten Mal!\"\n}\n\n# Funktion zur Generierung der nächsten Antwort basierend auf der Grammatik\ndef generate_response(user_input, step='<Start>', role='K'):\n    # Startet die Konversation mit der Useranfrage\n    response_chain = [user_input] if role == 'K' else []\n    \n    if step not in grammar:\n        response_chain.append(responses.get(step, \"[Keine Antwort verfügbar]\"))\n        return \" \".join(response_chain)\n    \n    # Auswahl eines Produktionspfads basierend auf Wahrscheinlichkeiten\n    production = random.choices(\n        grammar[step], \n        weights=[p[-1] for p in grammar[step]]\n    )[0]\n    \n    for symbol in production[:-1]:  # Exkludiert die Wahrscheinlichkeit am Ende\n        \n        if symbol in responses:\n            # Rolle berücksichtigen\n            if symbol.startswith(role):\n                response_chain.append(responses[symbol])\n            \n            # Rolle wechseln\n            role = 'V' if role == 'K' else 'K'\n            \n        else:\n            # Rekursive Verarbeitung, falls symbol nicht terminal ist\n            response_chain.append(generate_response(\"\", symbol, role))\n    \n    return \" \".join(response_chain)\n\n# Beispielhafte Nutzung der grammatikbasierten Antwort mit einer Kundenanfrage\nuser_input = \"Ich interessiere mich für das Produkt X.\"\ngenerated_text = generate_response(user_input)\n\n# Integration der Phase für das OpenAI-Modell basierend auf der Grammatikphase\nphase = \"Bedarfsanalyse\"  # Beispielhafte Phase aus der Grammatik\n\n# Erzeugt eine Systemnachricht basierend auf der Phase\nsystem_message = f\"Die Konversation befindet sich in der Phase: {phase}. Erörtern Sie die Anforderungen des Nutzers.\"\n\n# Aufbau der Anfrage an das LLM\nresponse = openai.ChatCompletion.create(\n  model=\"gpt-4\",\n  messages=[\n      {\"role\": \"system\", \"content\": system_message},\n      {\"role\": \"user\", \"content\": user_input}\n  ]\n)\n\n# Ausgabe der LLM-Antwort und der grammatikbasierten Antwort\nprint(\"Grammatikbasierte Antwort:\", generated_text)\nprint(\"LLM-Antwort:\", response.choices[0].message['content'])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Die Verwendung einer empirisch optimierten, probabilistischen Grammatik als Rahmen stellt tatsächlich einen **Zwischenweg** dar. Dieser Ansatz kombiniert strukturierte, regelbasierte Vorgaben mit den adaptiven, freien Antworten eines LLM. Dadurch können spezifische Kommunikationsmuster oder erwartete Gesprächsverläufe gezielt gefördert werden, ohne die volle Flexibilität des LLMs vollständig zu beschneiden. \n\n### Vorteile des Ansatzes als Zwischenweg:\n\n1. **Struktur und Fokus**: Die Grammatik setzt eine Struktur, die auf empirischen Daten basiert, was die Kommunikation zielgerichteter und kohärenter macht. So wird die Möglichkeit maximiert, dass das LLM innerhalb der gewünschten Gesprächslogik bleibt.\n\n2. **Erhaltung der Sprachvielfalt**: Da die Grammatik nur grundlegende Gesprächsphasen und Rollenwechsel definiert, kann das LLM innerhalb dieser Phasen immer noch vielfältige, nuancierte Antworten generieren. So bleibt es flexibel und kreativ.\n\n3. **Effizienz**: Die probabilistische Steuerung der Pfade in der Grammatik ermöglicht es dem LLM, häufiger realistische Antworten zu produzieren, die auf typischen Kommunikationsmustern basieren. Das reduziert die Wahrscheinlichkeit von themenfremden oder unpassenden Antworten und sorgt für eine dynamische, authentisch wirkende Unterhaltung.\n\n### Vergleich zur vollständigen Begrenzung:\nEin strikt regelbasierter Ansatz, der keine Flexibilität erlaubt, würde das LLM stark einschränken und könnte dazu führen, dass Konversationen unnatürlich wirken. Die probabilistische Grammatik schafft jedoch eine Balance, da sie nur grobe Strukturen vorgibt und das LLM innerhalb dieser Struktur frei reagieren kann. \n\n### Fazit:\nDiese Vorgehensweise ist kein künstliches Einengen, sondern eine Art Leitplanke, die ein **natürliches Gespräch unterstützt**, ohne die Tiefe und Nuancen des LLMs zu verlieren.\n\nDer Vergleich mit einem „Streckenfinder vor einem fahrenden PKW“ könnte in diesem Fall durchaus treffend sein, wenn man den Ansatz als unnötig kompliziert oder redundant betrachtet. Der Streckenfinder ist eine Technologie, die ursprünglich dazu gedacht war, ein Problem zu lösen, das mittlerweile durch GPS und andere Navigationssysteme weitgehend automatisiert und optimiert wurde. Übertragen auf das LLM (Large Language Model) und die Grammatik: Der probabilistische Ansatz könnte wie eine „extra Schicht“ wirken, die in manchen Fällen unnötig erscheint, da das LLM selbst auf Basis seiner Trainingsdaten bereits natürliche und relevante Antworten generieren kann.\n\nDennoch gibt es Situationen, in denen der „Streckenfinder“ durchaus nützlich sein kann. Zum Beispiel, wenn es darum geht, dem LLM gezielte Gesprächsstrukturen aufzuzwingen oder bestimmte Kommunikationsmuster hervorzuheben. In solchen Fällen könnte der Ansatz als zusätzliche Ebene der Kontrolle dienen, besonders wenn man sicherstellen möchte, dass das Gespräch der gewünschten Struktur folgt – etwa im Kundenservice oder Verkauf.\n\nDer Vergleich mit einem „Einweiser beim Einparken“ passt hier gut. Der Einweiser hilft dabei, ein Fahrzeug präzise in eine Parklücke zu manövrieren, besonders in engen oder schwierigen Situationen, wo der Fahrer allein möglicherweise Schwierigkeiten hat. Ähnlich kann eine empirisch optimierte Grammatik im Kontext eines LLM als eine Art „Einweiser“ wirken, um den Dialog in die gewünschte Richtung zu lenken. Sie sorgt dafür, dass das Modell nicht „aus der Spur gerät“, während es dennoch die Flexibilität und Kreativität beibehält, die das LLM auszeichnen.\n\nDer Ansatz des „Einweisens“ wird vor allem dann nützlich, wenn man sicherstellen möchte, dass das LLM einem spezifischen Gesprächsziel folgt. In weniger kontrollierten Kontexten könnte man jedoch auf diese Struktur verzichten und dem Modell mehr Freiraum für kreative und dynamische Antworten lassen.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "<?php\nsession_start();\n\n// Definierte Grammatik für die Gesprächsstruktur\n$grammar = [\n    '<Start>' => [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>' => [['KBG', 'VBG', 1.0]],\n    '<Bedarf>' => [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>' => [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>' => [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>' => [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n];\n\n// Antworten für die Gesprächskomponenten\n$responses = [\n    \"KBG\" => \"Guten Tag, können Sie mir helfen?\",\n    \"VBG\" => \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\" => \"Ich brauche frische Kirschen.\",\n    \"VBBd\" => \"Was genau benötigen Sie?\",\n    \"KBA\" => \"Die sind mir zu teuer.\",\n    \"VBA\" => \"Die sind preiswert.\",\n    \"KAA\" => \"Zu dem Preis nehme ich die Ware.\",\n    \"VAA\" => \"Zu dem Preis kann ich Ihnen die Ware anbieten.\",\n    \"KAV\" => \"Auf Wiedersehen!\",\n    \"VAV\" => \"Tschüss, bis zum nächsten Mal!\"\n];\n\n// Funktion zur Generierung der nächsten Antwort basierend auf der Grammatik\nfunction generate_response($user_input, $step = '<Start>', $role = 'K') {\n    global $grammar, $responses;\n\n    $response_chain = ($role == 'K') ? [$user_input] : [];\n\n    if (!isset($grammar[$step])) {\n        $response_chain[] = isset($responses[$step]) ? $responses[$step] : \"[Keine Antwort verfügbar]\";\n        return implode(\" \", $response_chain);\n    }\n\n    // Auswahl eines Produktionspfads basierend auf Wahrscheinlichkeiten\n    $production = random_choice($grammar[$step]);\n\n    foreach ($production as $symbol) {\n        if (isset($responses[$symbol])) {\n            if ($symbol[0] == $role) {\n                $response_chain[] = $responses[$symbol];\n            }\n            // Rolle wechseln\n            $role = ($role == 'K') ? 'V' : 'K';\n        } else {\n            $response_chain[] = generate_response(\"\", $symbol, $role);\n        }\n    }\n\n    return implode(\" \", $response_chain);\n}\n\n// Hilfsfunktion, um zufällig einen Pfad basierend auf Wahrscheinlichkeiten auszuwählen\nfunction random_choice($array) {\n    $total_weight = array_sum(array_column($array, 2));\n    $rand = mt_rand(0, $total_weight * 100) / 100;\n    $sum = 0;\n\n    foreach ($array as $item) {\n        $sum += $item[2];\n        if ($rand <= $sum) {\n            return $item[0];\n        }\n    }\n\n    return $array[0][0]; // Rückgabe des ersten Elements, falls etwas schief geht\n}\n\n// Initialisiere den API-Aufruf für OpenAI\n$openai_api_key = 'DEIN_API_KEY'; // Ersetze mit deinem OpenAI API-Schlüssel\n$llm_response = '';\n\n// Wenn ein Benutzereingabefeld ausgefüllt wurde, die Antwort generieren\nif ($_SERVER[\"REQUEST_METHOD\"] == \"POST\") {\n    $user_input = $_POST['user_input'];\n    $response = generate_response($user_input);\n    \n    // OpenAI API Anfrage\n    $llm_response = get_llm_response($user_input, \"Bedarfsanalyse\");\n}\n\n// Funktion zur Anfrage an OpenAI API\nfunction get_llm_response($user_input, $phase) {\n    global $openai_api_key;\n\n    $ch = curl_init();\n    curl_setopt($ch, CURLOPT_URL, \"https://api.openai.com/v1/chat/completions\");\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n    curl_setopt($ch, CURLOPT_POST, true);\n\n    $headers = [\n        \"Content-Type: application/json\",\n        \"Authorization: Bearer $openai_api_key\"\n    ];\n\n    $data = [\n        \"model\" => \"gpt-4\",\n        \"messages\" => [\n            [\"role\" => \"system\", \"content\" => \"Die Konversation befindet sich in der Phase: $phase. Erörtern Sie die Anforderungen des Nutzers.\"],\n            [\"role\" => \"user\", \"content\" => $user_input]\n        ]\n    ];\n\n    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);\n    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $response_data = json_decode($response, true);\n    return $response_data['choices'][0]['message']['content'] ?? 'Keine Antwort erhalten';\n}\n?>\n\n<!DOCTYPE html>\n<html lang=\"de\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Gesprächsführung mit LLM</title>\n</head>\n<body>\n    <h1>Willkommen zum Kundenservice</h1>\n    \n    <form method=\"POST\">\n        <label for=\"user_input\">Ihre Anfrage:</label>\n        <input type=\"text\" name=\"user_input\" id=\"user_input\" required>\n        <button type=\"submit\">Absenden</button>\n    </form>\n\n    <?php if ($llm_response): ?>\n        <h2>Grammatikbasierte Antwort:</h2>\n        <p><?php echo htmlspecialchars(generate_response($user_input)); ?></p>\n\n        <h2>LLM-Antwort:</h2>\n        <p><?php echo htmlspecialchars($llm_response); ?></p>\n\n        <form method=\"POST\">\n            <input type=\"hidden\" name=\"user_input\" value=\"<?php echo htmlspecialchars($user_input); ?>\">\n            <label for=\"user_input\">Neue Anfrage:</label>\n            <input type=\"text\" name=\"user_input\" id=\"user_input\" required>\n            <button type=\"submit\">Absenden</button>\n        </form>\n    <?php endif; ?>\n</body>\n</html>\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "\n### Erklärung des PHP-Skripts:\n\n1. **Grammatik und Antworten**:\n   - Die Grammatik für das Gespräch wird als PHP-Array gespeichert. Hier wird dieselbe Logik verwendet wie im Python-Code, um den Gesprächsfluss zu steuern.\n   \n2. **Antwortgenerierung**:\n   - Die Funktion `generate_response()` übernimmt die gleiche Funktion wie im Python-Skript und generiert Antworten basierend auf der definierten Grammatik. Eine zufällige Auswahl wird unter Berücksichtigung der Gewichtung der Optionen getroffen.\n\n3. **OpenAI-Integration**:\n   - Die OpenAI-API wird mit `curl` in PHP aufgerufen. Eine Anfrage wird erstellt, die das LLM auffordert, auf die Benutzereingabe zu antworten und den Dialog in der festgelegten \"Bedarfsanalyse\"-Phase fortzusetzen.\n   \n4. **Benutzeroberfläche**:\n   - Die Benutzeroberfläche besteht aus einem Formular zur Eingabe der Benutzeranfrage. Nach Absenden der Anfrage wird die Seite neu geladen und zeigt sowohl die grammatikbasierte Antwort als auch die Antwort des LLM an. Ein neues Eingabefeld ermöglicht die nächste Anfrage.\n\n5. **Formular mit verstecktem Feld**:\n   - Das Formular wird nach jeder Eingabe aktualisiert und das `user_input`-Feld mit der letzten Benutzeranfrage wird im Hintergrund gespeichert, um einen fortlaufenden Dialog zu ermöglichen.\n\nDieses PHP-Skript simuliert den Prozess der grammatikgesteuerten Antwortgenerierung und kombiniert dies mit der Leistungsfähigkeit eines LLM für die weitere Konversation.",
      "metadata": {}
    }
  ]
}