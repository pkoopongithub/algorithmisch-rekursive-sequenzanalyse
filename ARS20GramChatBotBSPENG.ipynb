{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Algorithmisch Rekursive Sequenzanalyse 2.0**  \n*Nutzen der optimierten Grammatik für einen Chatbot*  \nPaul Koop  \nNovember 2024  \npost@paul-koop.org",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Um ein erweitertes Python-Programm zu erstellen, das Open-Source-Modelle wie GPT-Neo oder GPT-J über Hugging Face nutzt, und mit einer spezifischen Grammatikstruktur arbeitet, wie du sie angegeben hast, gehen wir schrittweise vor. Das Programm wird Hugging Face's API verwenden, um die Modelle GPT-Neo oder GPT-J zu nutzen, und auf der Grundlage einer vorgegebenen Grammatik Anfragen generieren.\n\n### Voraussetzungen\n1. Installiere die Hugging Face `transformers`-Bibliothek:\n   ```bash\n   pip install transformers\n   ```\n\n2. Wenn du ein kostenloses Modell bei Hugging Face verwenden möchtest, benötigst du ein API-Schlüssel. Registriere dich bei [Hugging Face](https://huggingface.co/) und erstelle einen API-Schlüssel.\n\n3. Die Grammatik, die du bereitgestellt hast, legt fest, wie die Struktur von Konversationen aussieht. Wir werden die Grammatik implementieren und dann ein LLM-Modell verwenden, um die Antworten zu generieren.\n\n### Schritt 1: Grammatik definieren\nZunächst implementieren wir die Grammatik, die du bereitgestellt hast, und verwenden sie, um Konversationen zu generieren. Wir werden dies als eine Art \"Template\" für die Konversationslogik betrachten.\n\n### Schritt 2: API-Aufruf an Hugging Face\nWir werden die **GPT-Neo** oder **GPT-J** Modelle von Hugging Face verwenden, um auf die Anfragen basierend auf der Grammatik zu antworten.\n\nHier ist ein vollständiges Beispielprogramm in Python:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Set up the Hugging Face model and tokenizer (GPT-Neo oder GPT-J)\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"  # Du kannst auch \"EleutherAI/gpt-j-6B\" verwenden\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Die Grammatik für die Konversation\ngrammar = {\n    '<Start>': [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>': [['KBG', 'VBG', 1.0]],\n    '<Bedarf>': [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>': [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>': [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>': [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n}\n\n# Dummy-Daten für das Beispiel\nresponses = {\n    \"KBG\": \"Hallo! Wie kann ich Ihnen helfen?\",\n    \"VBG\": \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\": \"Ich kann Ihnen bei vielen Themen weiterhelfen.\",\n    \"VBBd\": \"Was genau benötigen Sie?\",\n    \"KBA\": \"Ich verstehe, dass Sie Hilfe mit etwas brauchen.\",\n    \"VBA\": \"Was kann ich konkret für Sie tun?\",\n    \"KAA\": \"Es freut mich, dass ich helfen konnte.\",\n    \"VAA\": \"Danke für Ihre Anfrage, gerne geschehen!\",\n    \"KAV\": \"Auf Wiedersehen!\",\n    \"VAV\": \"Tschüss, bis zum nächsten Mal!\"\n}\n\n# Funktion zur Auswahl eines Satzes basierend auf der Grammatik\ndef generate_sentence(grammar_key):\n    sentence_choice = random.choices(grammar[grammar_key], weights=[x[2] for x in grammar[grammar_key]])[0]\n    return [responses[part] for part in sentence_choice]\n\n# Funktion zur Generierung einer Konversation basierend auf der Grammatik\ndef generate_conversation():\n    conversation = generate_sentence('<Start>')\n    full_conversation = \" \".join(conversation)\n    return full_conversation\n\n# Funktion, um eine Anfrage an Hugging Face zu stellen\ndef get_model_response(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_length=150, num_return_sequences=1, temperature=0.7)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Beispiel, wie das Programm funktioniert\nif __name__ == \"__main__\":\n    print(\"Generierte Konversation (mit Grammatik):\")\n    conversation = generate_conversation()\n    print(conversation)\n\n    print(\"\\nAntwort vom LLM basierend auf der Anfrage:\")\n    response = get_model_response(conversation)\n    print(response)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "\n### Erklärung des Programms:\n1. **Grammatik**: Die Grammatikstruktur, die du vorgegeben hast, wird als ein **Dictionary** in Python implementiert. Es enthält alle möglichen Sätze, die durch bestimmte Schlüssel wie `<Start>`, `<Begrüßung>`, etc. definiert sind. Dabei sind die Sätze durch Wahrscheinlichkeiten gewichtet, um zu steuern, welche Elemente wahrscheinlicher erscheinen.\n\n2. **Antworten**: Die **`responses`** sind Beispielantworten für die verschiedenen grammatikalischen Strukturen. Diese werden im Template-System verwendet, um eine vollständige Konversation zu generieren.\n\n3. **Generierung der Konversation**: Die Funktion `generate_sentence` wählt mit zufälliger Gewichtung eine Kombination von Antworten aus, die dann eine Konversation im vorgegebenen Stil erzeugt. Diese Konversation wird mit der Grammatik in Einklang gebracht.\n\n4. **API-Aufruf an Hugging Face**: Die Funktion `get_model_response` stellt eine Anfrage an das GPT-Neo-Modell von Hugging Face. Sie nimmt die generierte Konversation und sendet sie als **Prompt** an das Modell. Die Antwort wird dann durch das Modell generiert und zurückgegeben.\n\n5. **Testlauf**: In der `main`-Funktion wird zunächst eine Konversation mit der Grammatik erzeugt, und danach wird diese Konversation an das LLM gesendet, um eine Antwort zu erhalten.\n\nDieses Beispiel zeigt, wie du ein einfaches System mit **Open-Source-LLMs** in Python einrichtest und Grammatik zur Strukturierung von Anfragen verwendest. Du kannst das Programm erweitern, indem du komplexere Grammatikstrukturen implementierst oder zusätzliche Modelle von Hugging Face ausprobierst.",
      "metadata": {}
    }
  ]
}