% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\subsubsection{\texorpdfstring{\textbf{Trägt die Nutzung von LLM in der
qualitativen Sozialforschung zum Verstehen
bei?}}{Trägt die Nutzung von LLM in der qualitativen Sozialforschung zum Verstehen bei?}}\label{truxe4gt-die-nutzung-von-llm-in-der-qualitativen-sozialforschung-zum-verstehen-bei}

Die Aussage, dass große Sprachmodelle (LLM) Dialoge imitieren, aber
nicht erklären, ist zentral für diese Frage. LLM sind in der Lage, die
Kontingenz und Opazität von Dialogen nachzubilden, da sie auf der
statistischen Analyse riesiger Textmengen basieren und
Wahrscheinlichkeiten für die Abfolge von Wörtern und Sätzen berechnen.
Sie können überzeugend menschlich klingende Dialoge generieren und sogar
Muster in qualitativen Daten identifizieren.

Wenn qualitative Sozialforscher ihre Arbeit von LLM erledigen lassen,
kann dies die Effizienz bei der Datenverarbeitung und Mustererkennung
erheblich steigern. LLM können große Mengen an Transkripten schnell
durchsuchen, Themen clustern oder erste Kategoriensysteme vorschlagen.

\textbf{Jedoch trägt dies nicht direkt zum tieferen \emph{Verstehen} im
Sinne der qualitativen Sozialforschung bei, aus folgenden Gründen:}

\begin{itemize}
\item
  \textbf{Imitation vs. Erklärung:} LLM sind im Kern
  Imitationsmaschinen. Sie reproduzieren vorhandene Muster, ohne ein
  \emph{Verständnis} der zugrunde liegenden sozialen Bedeutungen,
  Motivationen, Kontexte oder der intentionalen Handlungen der Akteure
  zu entwickeln. Das "Warum" oder "Wie" sozialer Phänomene, das
  qualitative Forschung anstrebt, bleibt für LLM unzugänglich.
\item
  \textbf{Opazität der LLM:} Die Funktionsweise von LLM ist selbst in
  hohem Maße opak ("Black Box"). Während sie Ergebnisse liefern, ist der
  Weg dorthin für den Forscher nicht transparent oder nachvollziehbar im
  Sinne einer menschlichen Interpretation.
\item
  \textbf{Fehlende kritische Reflexion:} Qualitative Sozialforschung
  erfordert kritische Reflexion des Forschers über eigene Vorannahmen,
  den Forschungsprozess und die gesellschaftlichen Implikationen der
  Ergebnisse. LLM können diese reflexive Ebene nicht leisten.
\item
  \textbf{Kontingenz der LLM-Ergebnisse:} Obwohl LLM Kontingenz in
  Dialogen imitieren können, sind ihre eigenen Ergebnisse kontingent in
  Bezug auf die Trainingsdaten und Algorithmen, was die
  Verallgemeinerbarkeit und die theoretische Fundierung ihrer
  "Erkenntnisse" einschränkt.
\end{itemize}

Die Nutzung von LLM kann ein \textbf{wertvolles Werkzeug} zur
Vorbereitung, Strukturierung und Unterstützung der qualitativen Analyse
sein, indem sie bestimmte Aufgaben automatisiert und neue Perspektiven
auf die Daten bietet. Das eigentliche Verstehen bleibt jedoch die Domäne
des menschlichen Forschenden, der die von LLM generierten Muster
interpretieren, kontextualisieren und theoretisch einordnen muss. Ohne
diese menschliche Interpretation bleiben die Ergebnisse der LLM
lediglich eine komplizierte Form der Mustererkennung.

\subsubsection{\texorpdfstring{\textbf{Vergleich mit der Algorithmisch
Rekursiven Sequenzanalyse (ARS) und ob ihre Ergebnisse eher erklärend
sind:}}{Vergleich mit der Algorithmisch Rekursiven Sequenzanalyse (ARS) und ob ihre Ergebnisse eher erklärend sind:}}\label{vergleich-mit-der-algorithmisch-rekursiven-sequenzanalyse-ars-und-ob-ihre-ergebnisse-eher-erkluxe4rend-sind}

Die Algorithmisch Rekursive Sequenzanalyse 2.0 (ARS 2.0), wie in den
hochgeladenen Dokumenten beschrieben, unterscheidet sich grundlegend von
LLM und kann als \textbf{eher erklärendes Modell} betrachtet werden.

\textbf{Vergleichspunkte:}

\begin{itemize}
\item
  \textbf{Fokus auf Grammatiken:} ARS 2.0 zielt darauf ab, eine
  \textbf{formale, probabilistische Grammatik} aus den sequenziellen
  Daten (z.B. Verkaufsgesprächen) zu induzieren. Eine Grammatik ist per
  Definition ein erklärendes Modell, da sie die Regeln und Strukturen
  festlegt, die die Generierung gültiger Sequenzen ermöglichen. Sie
  liefert ein explizites Modell der zugrunde liegenden
  Kommunikationsstruktur. LLM hingegen lernen keine expliziten
  Grammatiken im klassischen Sinne, sondern statistische
  Wahrscheinlichkeiten für Tokenabfolgen.
\item
  \textbf{Transparenz und Nachvollziehbarkeit:} Die Methodik von ARS 2.0
  ist transparent und nachvollziehbar. Die Schritte der
  Datenvorbereitung, Symbolzuweisung, Grammatikinduktion, Simulation und
  statistischen Validierung sind explizit definiert. Die induzierte
  Grammatik selbst ist ein interpretierbares Ergebnis, das als Hypothese
  über die Struktur der Kommunikation dient. Im Gegensatz dazu ist die
  interne Funktionsweise und Entscheidungsfindung eines LLM für den
  Benutzer undurchsichtig.
\item
  \textbf{Hypothesengenerierung und Testung:} ARS 2.0 arbeitet mit der
  Generierung von Hypothesen über die Struktur von Interaktionen, die
  dann durch die induzierte Grammatik formalisiert und durch den
  Vergleich mit empirischen Daten (z.B. Frequenzverteilungen,
  Korrelationsanalysen) statistisch getestet werden. Dies entspricht
  einem wissenschaftlichen Erklärungsansatz.
\item
  \textbf{Generative Fähigkeit als Erklärung:} Die Fähigkeit der
  induzierten Grammatik, künstliche, aber den empirischen Daten ähnliche
  Sequenzen zu generieren, ist ein Indiz für ihre Erklärungskraft. Wenn
  die Grammatik die beobachteten Muster erfolgreich reproduzieren kann,
  deutet dies darauf hin, dass sie die Regeln des Dialogs "verstanden"
  hat -- nicht im menschlichen Sinne, aber als formales Modell.
\item
  \textbf{Qualitative und quantitative Verbindung:} ARS 2.0 verbindet
  qualitative Einsichten (z.B. die Kategorisierung von
  Gesprächsbeiträgen) mit quantitativen Methoden (probabilistische
  Regeln, statistische Tests), um ein robustes und erklärendes Modell zu
  erstellen.
\end{itemize}

\textbf{Fazit:}

Während LLM Dialoge beeindruckend imitieren können, ohne die zugrunde
liegenden Mechanismen zu erklären, bietet die Algorithmisch Rekursive
Sequenzanalyse 2.0 ein \textbf{explizit erklärendes Modell} in Form
einer formalen Grammatik. Diese Grammatik legt die Regeln offen, nach
denen Dialoge aufgebaut sind, und ermöglicht es, Hypothesen über diese
Strukturen zu generieren und statistisch zu validieren. In diesem Sinne
trägt ARS 2.0 direkt zum \textbf{Verstehen der Struktur und Dynamik von
Dialogen} bei, indem sie ein transparentes und prüfbares
Erklärungsmodell liefert, das über die bloße Imitation hinausgeht.

\end{document}
