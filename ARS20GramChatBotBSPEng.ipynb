{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Algorithmic Recursive Sequence Analysis 2.0**\n*Using optimized grammar for a chatbot*\nPaul Koop\nNovember 2024\npost@paul-koop.org\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "To create an advanced Python program that uses open source models such as GPT-Neo or GPT-J via Hugging Face and works with a specific grammar structure as you specified, we will proceed step by step. The program will use Hugging Face's API to leverage the GPT-Neo or GPT-J models and generate requests based on a given grammar.\n\n### Requirements\n1. Install the Hugging Face `transformers` library:\n   ```bash\n   pip install transformers\n   ```\n\n2. If you want to use a free model on Hugging Face, you need an API key. Register with [Hugging Face](https://huggingface.co/) and create an API key.\n\n3. The grammar you provide determines what the structure of conversations looks like. We will implement the grammar and then use an LLM model to generate the answers.\n\n### Step 1: Define grammar\nFirst, we'll implement the grammar you provided and use it to generate conversations. We will consider this as a kind of \"template\" for conversation logic.\n\n### Step 2: API call to Hugging Face\nWe will use Hugging Face's **GPT-Neo** or **GPT-J** models to respond to the queries based on grammar.\n\nHere is a complete example program in Python:\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\n\n# Die Grammatik bleibt unverändert\ngrammar = { \n    '<Start>': [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>': [['KBG', 'VBG', 1.0]],\n    '<Bedarf>': [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>': [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>': [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>': [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n}\n\n# Die Antworten mit den angepassten Sätzen für K (Kunde) und V (Verkäufer)\nresponses = { \n    \"KBG\": \"Guten Tag, können Sie mir helfen?\",\n    \"VBG\": \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\": \"Ich brauche frische Kirschen.\",\n    \"VBBd\": \"Was genau benötigen Sie?\",\n    \"KBA\": \"Die sind mir zu teuer.\",\n    \"VBA\": \"Die sind preiswert.\",\n    \"KAA\": \"Zu dem Preis nehme ich die Ware.\",\n    \"VAA\": \"Zu dem Preis kann ich Ihnen die Ware anbieten.\",\n    \"KAV\": \"Auf Wiedersehen!\",\n    \"VAV\": \"Tschüss, bis zum nächsten Mal!\"\n}\n\n# Funktion zur Auswahl der nächsten Antwort basierend auf der Grammatik\ndef generate_response(step='<Start>', role='K'):\n    if step not in grammar:\n        return responses.get(step, \"[Keine Antwort verfügbar]\")\n    \n    # Auswahl eines Produktionspfads basierend auf Wahrscheinlichkeiten\n    production = random.choices(\n        grammar[step], \n        weights=[p[-1] for p in grammar[step]]\n    )[0]\n    \n    # Hole das nächste Symbol\n    response_chain = []\n    for symbol in production[:-1]:  # Exkludiert die Wahrscheinlichkeit am Ende\n        \n        if symbol in responses:\n            # Rolle berücksichtigen\n            if symbol.startswith(role):\n                response_chain.append(responses[symbol])\n            \n            # Rolle wechseln\n            role = 'V' if role == 'K' else 'K'\n            \n        else:\n            # Rekursive Verarbeitung, falls symbol nicht terminal ist\n            response_chain.append(generate_response(symbol, role))\n    \n    return \" \".join(response_chain)\n\n# Beispielhafte Nutzung\nprint(generate_response())\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Program Explanation\n\nThis program implements a basic dialog interaction between a customer (K) and a salesperson (V) using a predefined grammar and corresponding responses. The grammar structure and role distribution between customer and seller dictate the sequence of responses.\n\n### Explanation of Grammar Use\n\n1. **Role Switching**:\n   In the `generate_response` function, the roles `K` (customer) and `V` (salesperson) alternate. When the current role is `K`, the program selects a response for the customer and then switches to `V`, so that the next response comes from the salesperson. This alternating switch allows the conversation to resemble a realistic sales dialogue.\n\n2. **Production Based on Probabilities**:\n   Using the `random.choices()` function, the program selects a production path from the grammar based on predefined probabilities. As a result, each execution of the program generates a slightly different conversation flow depending on the probability of each production rule. This adds a degree of dynamism to the dialogue flow.\n\n3. **Recursion for Non-Terminal Symbols**:\n   If the current symbol is a non-terminal (e.g., `<BedarfSegment>`), the `generate_response` function calls itself recursively until it reaches a terminal symbol (an actual response). This recursive approach allows for the handling of complex, multi-step dialogues, ensuring that the sequence of customer and salesperson responses follows the grammar rules.\n\n4. **Combining Responses**:\n   Responses are collected in a list called `response_chain` and ultimately returned as a single formatted conversation string. This ensures that the entire dialogue is returned seamlessly and correctly formatted.\n\nWith this approach, the program dynamically adjusts responses according to the grammar structure and role sequence, producing a coherent dialogue based on the grammar rules.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The approach of using an empirically optimized grammar to guide an LLM is, in some ways, new and innovative. While traditional chatbots and rule-based systems often use fixed dialogue flows and predefined decision trees, modern LLM-based chatbots rely on flexible, context-driven responses that are based on probability distributions within the model. However, by using a targeted, optimized grammar, this approach aims to combine that flexibility with structured conversation guidance.\n\nIn recent years, there have been various approaches to guiding LLMs through rule-based systems or grammar-like structures. However, these systems were often used to restrict content or generate specialized, context-specific responses. An empirically optimized grammar —based on real conversation data and specifically used to describe and control the dialogue flow—combines the strengths of both approaches:\n\n1. **Maintaining natural dialogue dynamics**: The LLM brings the capability to respond flexibly to user queries without being trapped in fixed decision trees.\n  \n2. **Structure and guidance**: The grammar adds an extra level of control, prioritizing or making certain conversational patterns more likely. This can help guide the dialogue flow in a specific direction, for example, based on tested interactions or typical conversational strategies (as in sales dialogues).\n\n3. **Flexibility and adaptation to specific scenarios**: Through optimizing the probabilities, certain sequences and responses can be preferred, which is useful to meet expectations for a specific conversation structure (such as in sales conversations) without entirely losing the variability of an LLM.\n\nOverall, the approach of specifically using empirically optimized grammars to control an LLM-based dialogue system is an exciting attempt to balance flexibility and structure, and could be particularly promising for domain-specific applications, such as sales conversations, consultations, or support interactions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\nimport openai\n\n# Definierte Grammatik für die Gesprächsstruktur\ngrammar = { \n    '<Start>': [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>': [['KBG', 'VBG', 1.0]],\n    '<Bedarf>': [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>': [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>': [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>': [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n}\n\n# Antworten für die Gesprächskomponenten\nresponses = { \n    \"KBG\": \"Guten Tag, können Sie mir helfen?\",\n    \"VBG\": \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\": \"Ich brauche frische Kirschen.\",\n    \"VBBd\": \"Was genau benötigen Sie?\",\n    \"KBA\": \"Die sind mir zu teuer.\",\n    \"VBA\": \"Die sind preiswert.\",\n    \"KAA\": \"Zu dem Preis nehme ich die Ware.\",\n    \"VAA\": \"Zu dem Preis kann ich Ihnen die Ware anbieten.\",\n    \"KAV\": \"Auf Wiedersehen!\",\n    \"VAV\": \"Tschüss, bis zum nächsten Mal!\"\n}\n\n# Funktion zur Generierung der nächsten Antwort basierend auf der Grammatik\ndef generate_response(user_input, step='<Start>', role='K'):\n    # Startet die Konversation mit der Useranfrage\n    response_chain = [user_input] if role == 'K' else []\n    \n    if step not in grammar:\n        response_chain.append(responses.get(step, \"[Keine Antwort verfügbar]\"))\n        return \" \".join(response_chain)\n    \n    # Auswahl eines Produktionspfads basierend auf Wahrscheinlichkeiten\n    production = random.choices(\n        grammar[step], \n        weights=[p[-1] for p in grammar[step]]\n    )[0]\n    \n    for symbol in production[:-1]:  # Exkludiert die Wahrscheinlichkeit am Ende\n        \n        if symbol in responses:\n            # Rolle berücksichtigen\n            if symbol.startswith(role):\n                response_chain.append(responses[symbol])\n            \n            # Rolle wechseln\n            role = 'V' if role == 'K' else 'K'\n            \n        else:\n            # Rekursive Verarbeitung, falls symbol nicht terminal ist\n            response_chain.append(generate_response(\"\", symbol, role))\n    \n    return \" \".join(response_chain)\n\n# Beispielhafte Nutzung der grammatikbasierten Antwort mit einer Kundenanfrage\nuser_input = \"Ich interessiere mich für das Produkt X.\"\ngenerated_text = generate_response(user_input)\n\n# Integration der Phase für das OpenAI-Modell basierend auf der Grammatikphase\nphase = \"Bedarfsanalyse\"  # Beispielhafte Phase aus der Grammatik\n\n# Erzeugt eine Systemnachricht basierend auf der Phase\nsystem_message = f\"Die Konversation befindet sich in der Phase: {phase}. Erörtern Sie die Anforderungen des Nutzers.\"\n\n# Aufbau der Anfrage an das LLM\nresponse = openai.ChatCompletion.create(\n  model=\"gpt-4\",\n  messages=[\n      {\"role\": \"system\", \"content\": system_message},\n      {\"role\": \"user\", \"content\": user_input}\n  ]\n)\n\n# Ausgabe der LLM-Antwort und der grammatikbasierten Antwort\nprint(\"Grammatikbasierte Antwort:\", generated_text)\nprint(\"LLM-Antwort:\", response.choices[0].message['content'])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The use of an empirically optimized, probabilistic grammar as a framework indeed represents a middle ground. This approach combines structured, rule-based guidelines with the adaptive, free responses of an LLM. In doing so, specific communication patterns or expected conversation flows can be encouraged without completely restricting the LLM’s full flexibility.\n\n**Advantages of the Middle Ground Approach:**\n\n**Structure and Focus:** The grammar provides a structure based on empirical data, which makes communication more targeted and coherent. This maximizes the likelihood that the LLM remains within the desired conversational logic.\n\n**Preservation of Language Variety:** Since the grammar only defines basic conversation phases and role shifts, the LLM can still generate diverse, nuanced responses within these phases. This allows it to remain flexible and creative.\n\n**Efficiency:** The probabilistic control of the paths in the grammar enables the LLM to produce realistic responses that are based on typical communication patterns more frequently. This reduces the chance of off-topic or inappropriate responses, resulting in a dynamic, authentic conversation.\n\n**Comparison to Full Constraint:**\nA strictly rule-based approach that allows no flexibility would greatly restrict the LLM and could lead to conversations that feel unnatural. The probabilistic grammar, however, creates a balance by setting broad structures while allowing the LLM to respond freely within this framework.\n\n**Conclusion:**\nThis approach is not an artificial constraint but rather a kind of guideline that supports a natural conversation without losing the depth and nuances of the LLM.\n\nThe comparison with a \"route finder in front of a moving car\" might be fitting in this case if one considers the approach to be unnecessarily complicated or redundant. A route finder is a technology originally designed to solve a problem that has already been largely automated and optimized by GPS and other navigation systems. When applied to LLM (Large Language Models) and grammar, the probabilistic approach could seem like an \"extra layer,\" which may be unnecessary in some cases, since the LLM can already generate natural and relevant responses based on its training data.\n\nHowever, there are situations where the \"route finder\" can still be useful, especially when it comes to imposing specific conversation structures or emphasizing certain communication patterns. In such cases, the approach can serve as an additional layer of control, particularly when one wants to ensure that the conversation stays within the desired structure—such as in customer service or sales.\n\nThe comparison with a \"parking guide\" works well here. The parking guide helps maneuver a vehicle precisely into a parking spot, especially in tight or difficult situations where the driver might struggle alone. Similarly, an empirically optimized grammar in the context of an LLM can act as a kind of \"guide,\" directing the conversation in the desired direction. It ensures that the model doesn’t \"veer off track,\" while still maintaining the flexibility and creativity that define LLMs.\n\nThe \"guide\" approach is particularly useful when one wants to ensure that the LLM follows a specific conversational goal. In less controlled contexts, however, one could forgo this structure and give the model more freedom for creative and dynamic responses.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "<?php\nsession_start();\n\n// Definierte Grammatik für die Gesprächsstruktur\n$grammar = [\n    '<Start>' => [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>' => [['KBG', 'VBG', 1.0]],\n    '<Bedarf>' => [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>' => [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>' => [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>' => [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n];\n\n// Antworten für die Gesprächskomponenten\n$responses = [\n    \"KBG\" => \"Guten Tag, können Sie mir helfen?\",\n    \"VBG\" => \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\" => \"Ich brauche frische Kirschen.\",\n    \"VBBd\" => \"Was genau benötigen Sie?\",\n    \"KBA\" => \"Die sind mir zu teuer.\",\n    \"VBA\" => \"Die sind preiswert.\",\n    \"KAA\" => \"Zu dem Preis nehme ich die Ware.\",\n    \"VAA\" => \"Zu dem Preis kann ich Ihnen die Ware anbieten.\",\n    \"KAV\" => \"Auf Wiedersehen!\",\n    \"VAV\" => \"Tschüss, bis zum nächsten Mal!\"\n];\n\n// Funktion zur Generierung der nächsten Antwort basierend auf der Grammatik\nfunction generate_response($user_input, $step = '<Start>', $role = 'K') {\n    global $grammar, $responses;\n\n    $response_chain = ($role == 'K') ? [$user_input] : [];\n\n    if (!isset($grammar[$step])) {\n        $response_chain[] = isset($responses[$step]) ? $responses[$step] : \"[Keine Antwort verfügbar]\";\n        return implode(\" \", $response_chain);\n    }\n\n    // Auswahl eines Produktionspfads basierend auf Wahrscheinlichkeiten\n    $production = random_choice($grammar[$step]);\n\n    foreach ($production as $symbol) {\n        if (isset($responses[$symbol])) {\n            if ($symbol[0] == $role) {\n                $response_chain[] = $responses[$symbol];\n            }\n            // Rolle wechseln\n            $role = ($role == 'K') ? 'V' : 'K';\n        } else {\n            $response_chain[] = generate_response(\"\", $symbol, $role);\n        }\n    }\n\n    return implode(\" \", $response_chain);\n}\n\n// Hilfsfunktion, um zufällig einen Pfad basierend auf Wahrscheinlichkeiten auszuwählen\nfunction random_choice($array) {\n    $total_weight = array_sum(array_column($array, 2));\n    $rand = mt_rand(0, $total_weight * 100) / 100;\n    $sum = 0;\n\n    foreach ($array as $item) {\n        $sum += $item[2];\n        if ($rand <= $sum) {\n            return $item[0];\n        }\n    }\n\n    return $array[0][0]; // Rückgabe des ersten Elements, falls etwas schief geht\n}\n\n// Initialisiere den API-Aufruf für OpenAI\n$openai_api_key = 'DEIN_API_KEY'; // Ersetze mit deinem OpenAI API-Schlüssel\n$llm_response = '';\n\n// Wenn ein Benutzereingabefeld ausgefüllt wurde, die Antwort generieren\nif ($_SERVER[\"REQUEST_METHOD\"] == \"POST\") {\n    $user_input = $_POST['user_input'];\n    $response = generate_response($user_input);\n    \n    // OpenAI API Anfrage\n    $llm_response = get_llm_response($user_input, \"Bedarfsanalyse\");\n}\n\n// Funktion zur Anfrage an OpenAI API\nfunction get_llm_response($user_input, $phase) {\n    global $openai_api_key;\n\n    $ch = curl_init();\n    curl_setopt($ch, CURLOPT_URL, \"https://api.openai.com/v1/chat/completions\");\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n    curl_setopt($ch, CURLOPT_POST, true);\n\n    $headers = [\n        \"Content-Type: application/json\",\n        \"Authorization: Bearer $openai_api_key\"\n    ];\n\n    $data = [\n        \"model\" => \"gpt-4\",\n        \"messages\" => [\n            [\"role\" => \"system\", \"content\" => \"Die Konversation befindet sich in der Phase: $phase. Erörtern Sie die Anforderungen des Nutzers.\"],\n            [\"role\" => \"user\", \"content\" => $user_input]\n        ]\n    ];\n\n    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);\n    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $response_data = json_decode($response, true);\n    return $response_data['choices'][0]['message']['content'] ?? 'Keine Antwort erhalten';\n}\n?>\n\n<!DOCTYPE html>\n<html lang=\"de\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Gesprächsführung mit LLM</title>\n</head>\n<body>\n    <h1>Willkommen zum Kundenservice</h1>\n    \n    <form method=\"POST\">\n        <label for=\"user_input\">Ihre Anfrage:</label>\n        <input type=\"text\" name=\"user_input\" id=\"user_input\" required>\n        <button type=\"submit\">Absenden</button>\n    </form>\n\n    <?php if ($llm_response): ?>\n        <h2>Grammatikbasierte Antwort:</h2>\n        <p><?php echo htmlspecialchars(generate_response($user_input)); ?></p>\n\n        <h2>LLM-Antwort:</h2>\n        <p><?php echo htmlspecialchars($llm_response); ?></p>\n\n        <form method=\"POST\">\n            <input type=\"hidden\" name=\"user_input\" value=\"<?php echo htmlspecialchars($user_input); ?>\">\n            <label for=\"user_input\">Neue Anfrage:</label>\n            <input type=\"text\" name=\"user_input\" id=\"user_input\" required>\n            <button type=\"submit\">Absenden</button>\n        </form>\n    <?php endif; ?>\n</body>\n</html>\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Explanation of the PHP Script:**\n\n**Grammar and Responses:**\n\nThe grammar for the conversation is stored as a PHP array. The same logic as in the Python code is used to control the flow of the conversation.\n\n**Response Generation:**\n\nThe `generate_response()` function performs the same role as in the Python script, generating responses based on the defined grammar. A random selection is made, taking into account the weightings of the options.\n\n**OpenAI Integration:**\n\nThe OpenAI API is called using `curl` in PHP. A request is created asking the LLM to respond to the user's input and continue the dialogue in the specified \"needs analysis\" phase.\n\n**User Interface:**\n\nThe user interface consists of a form for entering the user's query. After submitting the query, the page is reloaded to display both the grammar-based response and the LLM response. A new input field allows for the next query.\n\n**Form with Hidden Field:**\n\nThe form is updated after each input, and the `user_input` field with the last user query is stored in the background to allow for a continuous dialogue.\n\nThis PHP script simulates the process of grammar-driven response generation and combines it with the power of an LLM for further conversation.",
      "metadata": {}
    }
  ]
}
