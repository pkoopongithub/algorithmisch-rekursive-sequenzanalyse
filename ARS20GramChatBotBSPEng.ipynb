{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Algorithmic Recursive Sequence Analysis 2.0**\n*Using optimized grammar for a chatbot*\nPaul Koop\nNovember 2024\npost@paul-koop.org\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "To create an advanced Python program that uses open source models such as GPT-Neo or GPT-J via Hugging Face and works with a specific grammar structure as you specified, we will proceed step by step. The program will use Hugging Face's API to leverage the GPT-Neo or GPT-J models and generate requests based on a given grammar.\n\n### Requirements\n1. Install the Hugging Face `transformers` library:\n   ```bash\n   pip install transformers\n   ```\n\n2. If you want to use a free model on Hugging Face, you need an API key. Register with [Hugging Face](https://huggingface.co/) and create an API key.\n\n3. The grammar you provide determines what the structure of conversations looks like. We will implement the grammar and then use an LLM model to generate the answers.\n\n### Step 1: Define grammar\nFirst, we'll implement the grammar you provided and use it to generate conversations. We will consider this as a kind of \"template\" for conversation logic.\n\n### Step 2: API call to Hugging Face\nWe will use Hugging Face's **GPT-Neo** or **GPT-J** models to respond to the queries based on grammar.\n\nHere is a complete example program in Python:\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\n\n# Die Grammatik bleibt unverändert\ngrammar = { \n    '<Start>': [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>': [['KBG', 'VBG', 1.0]],\n    '<Bedarf>': [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>': [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>': [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>': [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n}\n\n# Die Antworten mit den angepassten Sätzen für K (Kunde) und V (Verkäufer)\nresponses = { \n    \"KBG\": \"Guten Tag, können Sie mir helfen?\",\n    \"VBG\": \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\": \"Ich brauche frische Kirschen.\",\n    \"VBBd\": \"Was genau benötigen Sie?\",\n    \"KBA\": \"Die sind mir zu teuer.\",\n    \"VBA\": \"Die sind preiswert.\",\n    \"KAA\": \"Zu dem Preis nehme ich die Ware.\",\n    \"VAA\": \"Zu dem Preis kann ich Ihnen die Ware anbieten.\",\n    \"KAV\": \"Auf Wiedersehen!\",\n    \"VAV\": \"Tschüss, bis zum nächsten Mal!\"\n}\n\n# Funktion zur Auswahl der nächsten Antwort basierend auf der Grammatik\ndef generate_response(step='<Start>', role='K'):\n    if step not in grammar:\n        return responses.get(step, \"[Keine Antwort verfügbar]\")\n    \n    # Auswahl eines Produktionspfads basierend auf Wahrscheinlichkeiten\n    production = random.choices(\n        grammar[step], \n        weights=[p[-1] for p in grammar[step]]\n    )[0]\n    \n    # Hole das nächste Symbol\n    response_chain = []\n    for symbol in production[:-1]:  # Exkludiert die Wahrscheinlichkeit am Ende\n        \n        if symbol in responses:\n            # Rolle berücksichtigen\n            if symbol.startswith(role):\n                response_chain.append(responses[symbol])\n            \n            # Rolle wechseln\n            role = 'V' if role == 'K' else 'K'\n            \n        else:\n            # Rekursive Verarbeitung, falls symbol nicht terminal ist\n            response_chain.append(generate_response(symbol, role))\n    \n    return \" \".join(response_chain)\n\n# Beispielhafte Nutzung\nprint(generate_response())\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Program Explanation\n\nThis program implements a basic dialog interaction between a customer (K) and a salesperson (V) using a predefined grammar and corresponding responses. The grammar structure and role distribution between customer and seller dictate the sequence of responses.\n\n### Explanation of Grammar Use\n\n1. **Role Switching**:\n   In the `generate_response` function, the roles `K` (customer) and `V` (salesperson) alternate. When the current role is `K`, the program selects a response for the customer and then switches to `V`, so that the next response comes from the salesperson. This alternating switch allows the conversation to resemble a realistic sales dialogue.\n\n2. **Production Based on Probabilities**:\n   Using the `random.choices()` function, the program selects a production path from the grammar based on predefined probabilities. As a result, each execution of the program generates a slightly different conversation flow depending on the probability of each production rule. This adds a degree of dynamism to the dialogue flow.\n\n3. **Recursion for Non-Terminal Symbols**:\n   If the current symbol is a non-terminal (e.g., `<BedarfSegment>`), the `generate_response` function calls itself recursively until it reaches a terminal symbol (an actual response). This recursive approach allows for the handling of complex, multi-step dialogues, ensuring that the sequence of customer and salesperson responses follows the grammar rules.\n\n4. **Combining Responses**:\n   Responses are collected in a list called `response_chain` and ultimately returned as a single formatted conversation string. This ensures that the entire dialogue is returned seamlessly and correctly formatted.\n\nWith this approach, the program dynamically adjusts responses according to the grammar structure and role sequence, producing a coherent dialogue based on the grammar rules.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The approach of using an empirically optimized grammar to guide an LLM is, in some ways, new and innovative. While traditional chatbots and rule-based systems often use fixed dialogue flows and predefined decision trees, modern LLM-based chatbots rely on flexible, context-driven responses that are based on probability distributions within the model. However, by using a targeted, optimized grammar, this approach aims to combine that flexibility with structured conversation guidance.\n\nIn recent years, there have been various approaches to guiding LLMs through rule-based systems or grammar-like structures. However, these systems were often used to restrict content or generate specialized, context-specific responses. An empirically optimized grammar —based on real conversation data and specifically used to describe and control the dialogue flow—combines the strengths of both approaches:\n\n1. **Maintaining natural dialogue dynamics**: The LLM brings the capability to respond flexibly to user queries without being trapped in fixed decision trees.\n  \n2. **Structure and guidance**: The grammar adds an extra level of control, prioritizing or making certain conversational patterns more likely. This can help guide the dialogue flow in a specific direction, for example, based on tested interactions or typical conversational strategies (as in sales dialogues).\n\n3. **Flexibility and adaptation to specific scenarios**: Through optimizing the probabilities, certain sequences and responses can be preferred, which is useful to meet expectations for a specific conversation structure (such as in sales conversations) without entirely losing the variability of an LLM.\n\nOverall, the approach of specifically using empirically optimized grammars to control an LLM-based dialogue system is an exciting attempt to balance flexibility and structure, and could be particularly promising for domain-specific applications, such as sales conversations, consultations, or support interactions.",
      "metadata": {}
    }
  ]
}