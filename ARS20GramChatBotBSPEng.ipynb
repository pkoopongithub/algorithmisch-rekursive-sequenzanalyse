{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Algorithmic Recursive Sequence Analysis 2.0**\n*Using optimized grammar for a chatbot*\nPaul Koop\nNovember 2024\npost@paul-koop.org\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "To create an advanced Python program that uses open source models such as GPT-Neo or GPT-J via Hugging Face and works with a specific grammar structure as you specified, we will proceed step by step. The program will use Hugging Face's API to leverage the GPT-Neo or GPT-J models and generate requests based on a given grammar.\n\n### Requirements\n1. Install the Hugging Face `transformers` library:\n   ```bash\n   pip install transformers\n   ```\n\n2. If you want to use a free model on Hugging Face, you need an API key. Register with [Hugging Face](https://huggingface.co/) and create an API key.\n\n3. The grammar you provide determines what the structure of conversations looks like. We will implement the grammar and then use an LLM model to generate the answers.\n\n### Step 1: Define grammar\nFirst, we'll implement the grammar you provided and use it to generate conversations. We will consider this as a kind of \"template\" for conversation logic.\n\n### Step 2: API call to Hugging Face\nWe will use Hugging Face's **GPT-Neo** or **GPT-J** models to respond to the queries based on grammar.\n\nHere is a complete example program in Python:\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Set up the Hugging Face model and tokenizer (GPT-Neo oder GPT-J)\nmodel_name = \"EleutherAI/gpt-neo-2.7B\"  # Du kannst auch \"EleutherAI/gpt-j-6B\" verwenden\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Die Grammatik für die Konversation\ngrammar = {\n    '<Start>': [['<Begrüßung>', '<Bedarf>', '<Abschluss>', '<Verabschiedung>', 1.0]],\n    '<Begrüßung>': [['KBG', 'VBG', 1.0]],\n    '<Bedarf>': [['<BedarfSegment>', '<Bedarf>', 0.8], ['<BedarfSegment>', 0.2]],\n    '<BedarfSegment>': [['KBBd', 'VBBd', 0.4], ['KBBd', 'VBA', 0.3], ['KBA', 'VBA', 0.3]],\n    '<Abschluss>': [['KAA', 'VAA', 0.6], ['VAA', 'KAA', 0.4]],\n    '<Verabschiedung>': [['KAV', 'VAV', 0.7], ['VAV', 'KAV', 0.3]],\n}\n\n# Dummy-Daten für das Beispiel\nresponses = {\n    \"KBG\": \"Hallo! Wie kann ich Ihnen helfen?\",\n    \"VBG\": \"Guten Tag, was darf ich für Sie tun?\",\n    \"KBBd\": \"Ich kann Ihnen bei vielen Themen weiterhelfen.\",\n    \"VBBd\": \"Was genau benötigen Sie?\",\n    \"KBA\": \"Ich verstehe, dass Sie Hilfe mit etwas brauchen.\",\n    \"VBA\": \"Was kann ich konkret für Sie tun?\",\n    \"KAA\": \"Es freut mich, dass ich helfen konnte.\",\n    \"VAA\": \"Danke für Ihre Anfrage, gerne geschehen!\",\n    \"KAV\": \"Auf Wiedersehen!\",\n    \"VAV\": \"Tschüss, bis zum nächsten Mal!\"\n}\n\n# Funktion zur Auswahl eines Satzes basierend auf der Grammatik\ndef generate_sentence(grammar_key):\n    sentence_choice = random.choices(grammar[grammar_key], weights=[x[2] for x in grammar[grammar_key]])[0]\n    return [responses[part] for part in sentence_choice]\n\n# Funktion zur Generierung einer Konversation basierend auf der Grammatik\ndef generate_conversation():\n    conversation = generate_sentence('<Start>')\n    full_conversation = \" \".join(conversation)\n    return full_conversation\n\n# Funktion, um eine Anfrage an Hugging Face zu stellen\ndef get_model_response(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_length=150, num_return_sequences=1, temperature=0.7)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Beispiel, wie das Programm funktioniert\nif __name__ == \"__main__\":\n    print(\"Generierte Konversation (mit Grammatik):\")\n    conversation = generate_conversation()\n    print(conversation)\n\n    print(\"\\nAntwort vom LLM basierend auf der Anfrage:\")\n    response = get_model_response(conversation)\n    print(response)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Explanation of the program:\n1. **Grammar**: The grammar structure you specified is implemented as a **Dictionary** in Python. It contains all possible sentences defined by specific keys like `<Start>`, `<Greeting>`, etc. The sentences are weighted by probabilities to control which elements appear more likely.\n\n2. **Answers**: The **`responses`** are example answers for the different grammatical structures. These are used in the template system to generate a complete conversation.\n\n3. **Conversation generation**: The `generate_sentence` function selects a combination of answers with random weighting, which then generates a conversation in the given style. This conversation is aligned with grammar.\n\n4. **API call to Hugging Face**: The `get_model_response` function makes a request to the GPT-Neo model of Hugging Face. It takes the generated conversation and sends it to the model as a **prompt**. The response is then generated and returned by the model.\n\n5. **Test Run**: In the `main` function, a conversation is first created with the grammar and after that this conversation is sent to the LLM to get a response.\n\n### Issues:\n1. **Grammar Complexity**: The grammar structure you provided is relatively simple and based on a fixed sentence structure. The actual LLM will then reinforce the grammar through the queries, but there is no guarantee that the model will be perfectly based on the grammar structure.\n2. **API Limitations**: Use of the Hugging Face API has a free tier, and free accounts often have limits on the number of requests per month. A paid model would be required for extensive use.\n\nThis example shows how to set up a simple system with **open source LLMs** in Python and use grammar to structure requests. You can extend the program by implementing more complex grammar structures or trying out additional models of Hugging Face.\n",
      "metadata": {}
    }
  ]
}