## *Algorithmic Recursive Sequence Analysis (ARS)*

### **Core Principles**


Algorithmic Recursive Sequence Analysis (ARS) is a method for analyzing discrete, finite symbol sequences derived from empirical data. It aims to construct a probabilistic context-free grammar (PCFG) that describes recurring structural patterns and generates synthetic sequences with similar properties. ARS is designed to integrate formal modeling with interpretive openness.


### **Methodological Steps**


1. **Exploratory Hypothesis Formation**

   * Based on transcriptions (e.g., conversations, observations), initial hypotheses and symbolic categories are developed to describe structural features (e.g., greeting, request, response, closing).
   * These hypotheses serve as flexible guides for inductive analysis rather than rigid constraints.

2. **Symbol Assignment and Sequence Encoding**

   * Communicative or behavioral elements are categorized and mapped to terminal symbols.
   * The resulting symbolic sequences are used for structural analysis.

3. **Grammar Induction**

   * A PCFG is inferred to model transitions and repetitions within the sequence data.
   * Parsing is used to assess the model’s coverage and consistency with the data.

4. **Synthetic Sequence Generation and Comparison**

   * Artificial sequences generated by the grammar are statistically compared to the empirical data to assess representativeness.

5. **Iterative Optimization**

   * Probabilities within the grammar are refined to better reflect the statistical properties of the source data.
   * Statistical measures (e.g., Spearman correlation, significance levels) support the evaluation of model fit.

6. **Simulation of Interactional Scenarios**

   * Multi-agent simulations use the grammar to explore behavioral variations and possible trajectories under different conditions.

---

### **Technical Implementation**

* **Grammar Inductor**: Automated grammar construction (e.g., Python, Scheme)
* **Parser**: Sequence validation (e.g., Pascal, Python)
* **Transducer**: Synthetic sequence generation (e.g., Lisp, Scheme)

At the core of this work lies the development and optimization of probabilistic context-free grammars (PCFGs) based on empirical transcripts – exemplified here using sales conversations.

Particularly noteworthy is the methodological integration of qualitative structural reconstruction with quantitative validation procedures (e.g., Spearman correlation, significance tests, Markov analysis). The iterative optimization of the grammar, combined with statistical feedback on empirical fit, constitutes a remarkable innovation. It enables not only the structured analysis of social interactions but also their simulation through multi-agent systems – with promising applications in consulting, education, and interface design.

The methodological implementation is transparent, formally sound, and characterized by a high degree of interdisciplinary compatibility (linguistics, computer science, sociology).


---

### **Theoretical Orientation**

ARS follows a formal-empirical paradigm, informed by computational modeling.
It is especially suited for researchers aiming to complement qualitative insights with structural models.

---

## *Algorithmisch-Rekursive Sequenzanalyse (ARS)*

### **Grundprinzipien**

Die Algorithmisch-Rekursive Sequenzanalyse (ARS) ist eine Methode zur Analyse diskreter, endlicher Symbolfolgen, die aus empirischen Daten gewonnen werden. Ziel ist die Konstruktion einer probabilistischen kontextfreien Grammatik (PCFG), die wiederkehrende Muster erfasst und synthetische Sequenzen mit vergleichbaren Eigenschaften erzeugt. ARS verbindet formale Modellierung mit interpretativer Offenheit.

### **Methodologische Schritte**

1. **Explorative Hypothesenbildung**

   * Auf Basis von Transkriptionen (z. B. Gespräche, Beobachtungen) werden erste Hypothesen und symbolische Kategorien entwickelt, um strukturelle Merkmale zu erfassen (z. B. Begrüßung, Bitte, Antwort, Abschluss).
   * Diese Hypothesen dienen als heuristische Orientierung, nicht als starre Vorgaben.

2. **Symbolzuweisung und Sequenzkodierung**

   * Kommunikative oder beobachtbare Einheiten werden Kategorien zugeordnet und in Terminalzeichen überführt.
   * Die entstehenden symbolischen Sequenzen dienen der Strukturanalyse.

3. **Grammatikinduktion**

   * Eine PCFG wird aus den Sequenzen abgeleitet, um Übergänge und Wiederholungen zu modellieren.
   * Durch Parsen wird geprüft, ob die Grammatik die empirischen Daten abdeckt.

4. **Generierung und Vergleich künstlicher Sequenzen**

   * Mit der Grammatik erzeugte Sequenzen werden statistisch mit den Originaldaten verglichen, um ihre Repräsentativität zu prüfen.

5. **Iterative Optimierung**

   * Die Wahrscheinlichkeiten in der Grammatik werden so angepasst, dass sie die Struktur der Ausgangsdaten bestmöglich widerspiegeln.
   * Statistische Kennzahlen (z. B. Spearman-Korrelation, Signifikanztests) unterstützen die Modellbewertung.

6. **Simulation interaktionaler Szenarien**

   * In Multiagentensimulationen nutzen Agenten die Grammatikregeln, um Verhaltensvariationen und hypothetische Verläufe zu untersuchen.

---

### **Technische Umsetzung**

* **Grammatikinduktor**: Automatische Grammatikerstellung (z. B. in Python, Scheme)
* **Parser**: Prüfung auf Wohlgeformtheit (z. B. in Pascal, Python)
* **Transduktor**: Generierung künstlicher Sequenzen (z. B. in Lisp, Scheme)

Im Zentrum steht die Entwicklung und Optimierung probabilistischer kontextfreier Grammatiken (PCFG) auf Grundlage empirischer Transkripte – exemplarisch am Beispiel von Verkaufsgesprächen demonstriert.

Besonders hervorzuheben ist die methodische Verbindung von qualitativer Strukturrekonstruktion mit quantitativen Validierungsverfahren (z. B. Spearman-Korrelation, Signifikanztests, Markov-Analyse). Die iterative Grammatikoptimierung in Kombination mit der statistischen Rückmeldung zur empirischen Passung stellt eine bemerkenswerte Innovation dar. Sie ermöglicht nicht nur die strukturierte Analyse sozialer Interaktionen, sondern auch deren Simulation durch Multiagentensysteme – mit Potenzial für Anwendungen in Beratung, Lehre und Interfacedesign.

Die methodische Umsetzung ist nachvollziehbar, formal korrekt und mit hoher interdisziplinärer Anschlussfähigkeit (Sprachwissenschaft, Informatik, Soziologie) ausgestattet.

---

### **Theoretische Ausrichtung**

ARS folgt einem formalen, empirischen Paradigma, das auf rechnergestützte Modellierung setzt.
Die Methode eignet sich insbesondere für Forschende, die qualitative Einsichten durch strukturelle Modelle ergänzen wollen.


---


**Algorithmisch Rekursive Sequenzanalyse 2.0**

#### Analyse der Verkaufsgespräche und Optimierung der Grammatik

- **Acht Transkripte für Grammatikinduktion**:
  - [ARS20AchtTranskripte.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20AchtTranskripte.ipynb)
  - [ARSEightTranscripts.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARSEightTranscripts.ipynb)
  - [ars20achttranskripte.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ars20achttranskripte.pdf)
  - [arseighttranscripts.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/arseighttranscripts.pdf)
  
  Acht Verkaufsgespräche wurden transkribiert und zur Grammatikinduktion verwendet, um eine Handlungsgrammatik für Verkaufsgespräche zu entwickeln. Die Übergangswahrscheinlichkeiten der resultierenden Grammatik wurden mit Python optimiert.

- **Intensive Analyse und Implementierung**:
  - [ARS20BeispielEng.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20BeispielEng.pdf)
  - [ARS20BeispielGer.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20BeispielGer.pdf)

  Ein einzelnes Transkript wurde anhand einer hypothetischen Grammatik detailliert analysiert. Dabei kamen verschiedene Werkzeuge zum Einsatz: Scheme für die Grammatikinduktion, Lisp für den Parser und Transduktor, sowie ein Multi-Agenten-System-Entwurf mit Python.

-

- **Optimierung der Übergangswahrscheinlichkeiten**:
  - [ARS20GramOpt.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20GramOpt.ipynb)
  - [ARS20GramOpt.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20GramOpt.pdf)
  - [ARS20GramOptEng.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20GramOptEng.ipynb)
  - [ARS2GgramOptEng.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS2GgramOptEng.pdf)

  Diese Dateien enthalten die Optimierung der Übergangswahrscheinlichkeiten mittels Python.



- **Konzeptionelle Überlegungen zu LLM und MAS**:
  - [QualitativeSocialResearchAndLargeLanguageModel.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/QualitativeSocialResearchAndLargeLanguageModel.ipynb)
  - [QualitativeSozialforschungUndGrossesSprachmodell.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/QualitativeSozialforschungUndGrossesSprachmodell.ipynb)
  - [QualitativeSozialforschungUndGrossesSprachmodellInduktorTransduktorParserMAS.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/QualitativeSozialforschungUndGrossesSprachmodellInduktorTransduktorParserMAS.ipynb)
  - [QualitativeSocialResearchAndLargeLanguageModelInductorTransductorParserMAS.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/QualitativeSocialResearchAndLargeLanguageModelInductorTransductorParserMAS.ipynb)
  - [VKGKausalinferenzSozialeStrukturenUndProzesse.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGKausalinferenzSozialeStrukturenUndProzesse.ipynb)
  - [VKGKausalinferenzSozialeStrukturenUndProzesse.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGKausalinferenzSozialeStrukturenUndProzesse.pdf)
  - [VKGMASKausalinferenzSozialeStrukturenUndProzesse.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGMASKausalinferenzSozialeStrukturenUndProzesse.ipynb)
  - [VKGMASKausalinferenzSozialeStrukturenUndProzesse.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGMASKausalinferenzSozialeStrukturenUndProzesse.pdf)
  - [VKGUeberblickInduktorParserMAS.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGUeberblickInduktorParserMAS.ipynb)
  - [VKGbayesPython-3.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGbayesPython-3.ipynb)
  - [VKGmultiagentensystem.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGmultiagentensystem.ipynb)

  Diese Dateien enthalten Überlegungen zu großen Sprachmodellen (LLM), Multi-Agenten-Systemen (MAS) und Kausalinferenz im Zusammenhang mit ARS.





- **Comparative Analysis and Theoretical Frameworks**:
  - [ARSvergleichMitAnderenMethoden.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARSvergleichMitAnderenMethoden.pdf)
  - [ARSNaturalismusKonstruktivismusPostmodernismus.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARSNaturalismusKonstruktivismusPostmodernismus.pdf)

  These documents provide a thorough comparative analysis of the ARS method in relation to other established approaches. The first document explores various methodologies, highlighting the strengths and weaknesses of each in a detailed comparison. The second document delves into the theoretical foundations of the ARS method, contrasting it with perspectives from Naturalism, Constructivism, and Postmodernism. The analyses are supported by interdisciplinary tools and frameworks, including statistical models, conceptual diagrams, and philosophical critiques.



- **Vergleichende Analyse und theoretische Rahmen**:
  - [ARSvergleichMitAnderenMethoden.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARSvergleichMitAnderenMethoden.pdf)
  - [ARSNaturalismusKonstruktivismusPostmodernismus.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARSNaturalismusKonstruktivismusPostmodernismus.pdf)

  Diese Dokumente bieten eine umfassende vergleichende Analyse der ARS-Methode im Verhältnis zu anderen etablierten Ansätzen. Das erste Dokument untersucht verschiedene Methodologien und beleuchtet die jeweiligen Stärken und Schwächen im Detailvergleich. Das zweite Dokument setzt sich mit den theoretischen Grundlagen der ARS-Methode auseinander und stellt diese den Perspektiven des Naturalismus, Konstruktivismus und Postmodernismus gegenüber. Die Analysen werden durch interdisziplinäre Werkzeuge und Rahmenwerke unterstützt, einschließlich statistischer Modelle, konzeptioneller Diagramme und philosophischer Kritiken.


### Algorithmisch Rekursive Sequenzanalyse (ARS 1994)

#### Ergebnisse und Implementierungen aus 1994

- **Zusammenfassung der Ergebnisse von ARS 1994**:
  - [PaulKoopARS_Attachments.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_Attachments.pdf)
  - [PaulKoopARS_English_Latex.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_English_Latex.pdf)
  - [PaulKoopARS_chinese.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_chinese.pdf)
  - [PaulKoopARS_english.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_english.pdf)
  - [PaulKoopARS_english_Attachments.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_english_Attachments.pdf)
  - [PaulKoopARS_french.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_french.pdf)
  - [PaulKoopARS_german.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_german.pdf)
  - [PaulKoopARS_german_Attachments.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_german_Attachments.pdf)
  - [PaulKoopARS_german_Latex.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_german_Latex.pdf)
  - [PaulKoopARS_russian.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_russian.pdf)
  - [PaulKoopARS_spain.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_spain.pdf)

  Diese Dokumente fassen die Ergebnisse des ARS-Projekts von 1994 zusammen.

- **Implementierungen in Lisp und Scheme**:
  - [VKG2.lsp](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKG2.lsp)
  - [vkgEnglishComment.lsp](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgEnglishComment.lsp)
  - [vkgGermanComment.lsp](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgGermanComment.lsp)
  - [VKG4.scm](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKG4.scm)
  - [vkgInductorEnglishComment.scm](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgInductorEnglishComment.scm)
  - [vkgInductorGermanComment.scm](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgInductorGermanComment.scm)

  Der Transduktor wurde in Lisp implementiert, während der Induktor in Scheme entwickelt wurde.

- **Parser und zugehörige Materialien**:
  - [VKGKORPUS.TXT](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGKORPUS.TXT)
  - [VKGPARSER.png](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGPARSER.png)
  - [LEXIKONVKG.ASC](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/LEXIKONVKG.ASC)
  - [PARSERVKG.PAS](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PARSERVKG.PAS)
  - [vkgParser.lsp](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgParser.lsp)

  Diese Dateien enthalten den Parser in Pascal und Lisp sowie zugehörige Materialien.

- **Texte und Forschungsdokumente zu ARS 1994**:
  - [fallstruktur.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/fallstruktur.pdf)
  - [methodologie.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/methodologie.pdf)
  - [oechsle.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/oechsle.pdf)
  - [reku.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/reku.pdf)
  - [GTGentscheidbarkeit.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/GTGentscheidbarkeit.pdf)
  - [KSysteme.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/KSysteme.pdf)

  Diese PDFs bieten zusätzliche Informationen zur Methodologie und zu den Ergebnissen von ARS 1994.

- **Tonbandprotokolle von 1994**:
  - [vkg1.mp3](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkg1.mp3)
  - [vkg2.mp3](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkg2.mp3)
  - [Aachen_280694_11Uhr.mp3](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/Aachen_280694_11Uhr.mp3)

  Diese Audiodateien enthalten das Tonbandprotokoll des Transkripts „Text 4“ sowie die acht Tonbandaufnahmen aus dem Jahr 1994.


  ## Algorithmic Recursive Sequence Analysis 2.0 (ARS 2024)

### Sales Dialogue Analysis and Grammar Induction

- [ARS20AchtTranskripte.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20AchtTranskripte.ipynb)
- [ARSEightTranscripts.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARSEightTranscripts.ipynb)
- [ars20achttranskripte.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ars20achttranskripte.pdf)
- [arseighttranscripts.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/arseighttranscripts.pdf)

Eight transcripts of sales dialogues are used for grammar induction to create an action grammar specific to sales conversations. The transition probabilities of the induced grammar are optimized using Python.

- [ARS20BeispielEng.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20BeispielEng.pdf)
- [ARS20BeispielGer.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20BeispielGer.pdf)

A single transcript is intensively analyzed based on a prior analysis and the creation of a hypothetical grammar. This involves grammar induction with Scheme, parsing and transduction using Lisp, and the development of a multi-agent system (MAS) with Python.



- [ARS20GramOpt.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20GramOpt.ipynb)
- [ARS20GramOpt.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20GramOpt.pdf)
- [ARS20GramOptEng.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS20GramOptEng.ipynb)
- [ARS2GgramOptEng.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/ARS2GgramOptEng.pdf)

Optimization of grammar transition probabilities (Python).



### Qualitative Social Research and Large Language Models

- [QualitativeSocialResearchAndLargeLanguageModel.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/QualitativeSocialResearchAndLargeLanguageModel.ipynb)
- [QualitativeSozialforschungUndGrossesSprachmodell.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/QualitativeSozialforschungUndGrossesSprachmodell.ipynb)
- [QualitativeSozialforschungUndGrossesSprachmodellInduktorTransduktorParserMAS.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/QualitativeSozialforschungUndGrossesSprachmodellInduktorTransduktorParserMAS.ipynb)
- [QualitativeSocialResearchAndLargeLanguageModelInductorTransductorParserMAS.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/QualitativeSocialResearchAndLargeLanguageModelInductorTransductorParserMAS.ipynb)
- [VKGKausalinferenzSozialeStrukturenUndProzesse.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGKausalinferenzSozialeStrukturenUndProzesse.ipynb)
- [VKGKausalinferenzSozialeStrukturenUndProzesse.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGKausalinferenzSozialeStrukturenUndProzesse.pdf)
- [VKGMASKausalinferenzSozialeStrukturenUndProzesse.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGMASKausalinferenzSozialeStrukturenUndProzesse.ipynb)
- [VKGMASKausalinferenzSozialeStrukturenUndProzesse.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGMASKausalinferenzSozialeStrukturenUndProzesse.pdf)
- [VKGUeberblickInduktorParserMAS.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGUeberblickInduktorParserMAS.ipynb)
- [VKGbayesPython-3.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGbayesPython-3.ipynb)
- [VKGmultiagentensystem.ipynb](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGmultiagentensystem.ipynb)

Conceptual considerations on large language models (LLMs), multi-agent systems (MAS), and causal inference in relation to ARS.

## Algorithmic Recursive Sequence Analysis (ARS 1994)

### Summary of ARS 1994 Results

- [PaulKoopARS_Attachments.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_Attachments.pdf)
- [PaulKoopARS_English_Latex.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_English_Latex.pdf)
- [PaulKoopARS_chinese.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_chinese.pdf)
- [PaulKoopARS_english.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_english.pdf)
- [PaulKoopARS_english_Attachments.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_english_Attachments.pdf)
- [PaulKoopARS_french.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_french.pdf)
- [PaulKoopARS_german.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_german.pdf)
- [PaulKoopARS_german_Attachments.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_german_Attachments.pdf)
- [PaulKoopARS_german_Latex.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_german_Latex.pdf)
- [PaulKoopARS_russian.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_russian.pdf)
- [PaulKoopARS_spain.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PaulKoopARS_spain.pdf)

Documents summarizing the results of ARS 1994.

### Grammar Tools and Analysis Components

- [VKG2.lsp](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKG2.lsp)
- [vkgEnglishComment.lsp](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgEnglishComment.lsp)
- [vkgGermanComment.lsp](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgGermanComment.lsp)

The transducer in Lisp.

- [VKG4.scm](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKG4.scm)
- [vkgInductorEnglishComment.scm](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgInductorEnglishComment.scm)
- [vkgInductorGermanComment.scm](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgInductorGermanComment.scm)

The grammar inducer in Scheme.

- [VKGKORPUS.TXT](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGKORPUS.TXT)
- [VKGPARSER.png](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/VKGPARSER.png)
- [LEXIKONVKG.ASC](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/LEXIKONVKG.ASC)
- [PARSERVKG.PAS](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/PARSERVKG.PAS)
- [vkgParser.lsp](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkgParser.lsp)

The parser (Pascal and Lisp).

### Supplementary Texts and Audio Recordings

- [fallstruktur.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/fallstruktur.pdf)
- [methodologie.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/methodologie.pdf)
- [oechsle.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/oechsle.pdf)
- [reku.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/reku.pdf)
- [GTGentscheidbarkeit.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/GTGentscheidbarkeit.pdf)
- [KSysteme.pdf](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/KSysteme.pdf)

Texts related to ARS 1994.

- [vkg1.mp3](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkg1.mp3)
- [vkg2.mp3](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/vkg2.mp3)
- [Aachen_280694_11Uhr.mp3](https://github.com/pkoopongithub/algorithmisch-rekursive-sequenzanalyse/blob/main/Aachen_280694_11Uhr.mp3)

Audio recordings of "Text 4" transcript and all eight recordings from 1994.





**Algorithmisch Rekursive Sequenzanalyse 2.0**

Large Language Models (LLMs), die Interaktionen imitieren und Muster erkennen können, eröffnen interessante neue Möglichkeiten für die soziologische Erforschung von Interaktionssettings. LLMs und das hier besprochene Verfahren können sich tatsächlich gegenseitig ergänzen, anstatt sich auszuschließen. Hier sind einige Überlegungen dazu, wie das Verfahren erweitert werden könnte, um von LLMs zu profitieren:

### 1. **Interpretation und Kategorienbildung durch LLMs als Unterstützung bei der Grammatikinduktion**
   LLMs könnten tatsächlich dabei helfen, in großen Transkriptkorpora Kategoriensymbole für unterschiedliche Satzarten und Gesprächsakte (z. B. Fragen, Bitten, Antworten, Einwände) zu identifizieren. Die Modelle sind in der Lage, eine große Bandbreite an Interaktionen zu imitieren und kontextualisierte Bedeutungen zu verstehen. Durch die Generierung solcher Kategoriensymbole könnten LLMs als "Vorkategorisierer" fungieren, der die Grundlage für eine tiefergehende, strukturelle Analyse bildet.

### 2. **Automatisierte und transparente Kategorienbildung**
   Ein Vorteil des LLM-Einsatzes wäre, dass die Kategorisierungslogik automatisiert und flexibel auf große Datenmengen anwendbar ist. Die Opazität der internen Regeln der LLMs bleibt zwar bestehen, aber durch spezifisches Fine-Tuning und Vergleich zwischen menschlicher und maschineller Kategorisierung könnten Forscher eine verlässliche, empirisch fundierte Basis für Kategorien schaffen, die in eine klar strukturierte Grammatik überführt werden kann.

### 3. **Ketten von Kategoriensymbolen für probabilistische Grammatikinduktion**
   Die von LLMs generierten Ketten von Kategoriensymbolen könnten dann als Eingabe für die probabilistische Grammatikinduktion dienen. Diese Grammatikinduktion könnte die Wahrscheinlichkeit und Häufigkeit von Gesprächsabläufen modellieren und Aussagen über Muster in verschiedenen Interaktionssettings treffen. Das heißt, die Grammatik könnte ein probabilistisches Modell aufbauen, das empirische und künstlich erzeugte Gesprächsverläufe vergleicht.

### 4. **Vergleich empirischer und künstlicher Kategorienketten zur Überprüfung von Modellen**
   Die Kombination von empirisch erfassten Kategorienketten mit künstlich erzeugten Ketten könnte als Validierungsmethode für LLMs dienen. Durch den Abgleich von beobachteten Gesprächsmustern in der Realität mit den synthetisch generierten könnten soziologische Forscher Rückschlüsse auf die Realitätsnähe und die Grenzen der LLM-Simulationen ziehen.

### 5. **Erforschung und Vergleich verschiedener Interaktionssettings**
   Da LLMs spezifische Gesprächskontexte imitieren können, könnten Forscher verschiedene künstliche Gesprächskontexte (z. B. Verkaufsgespräch, Beratungsgespräch) generieren lassen und diese dann mit empirischen Daten aus ähnlichen Settings vergleichen. Durch den Einsatz der probabilistischen Grammatikinduktion könnten Unterschiede oder Gemeinsamkeiten in Interaktionsmustern zwischen den künstlich generierten und den realen Daten identifiziert und soziologisch interpretiert werden.

### 6. **Überprüfung und Weiterentwicklung soziologischer Hypothesen**
   Indem sie künstlich erzeugte Kategorienketten zur Modellierung realer Interaktionen nutzen, könnten Forscher Hypothesen über Gesprächsstrukturen, Machtverhältnisse oder andere Interaktionsdynamiken überprüfen und weiterentwickeln. Sie könnten beispielsweise untersuchen, ob die Häufigkeit bestimmter Muster von Interaktionen in künstlich erzeugten Kontexten ähnlich verteilt ist wie in der Realität und wo signifikante Abweichungen bestehen.

### Fazit
Das Verfahren ist durch den Einsatz von LLMs keineswegs überholt, sondern kann durch sie ausgebaut werden. LLMs könnten in der Vorverarbeitung und in der automatisierten Kategorisierung von Gesprächssequenzen hilfreich sein. In der zweiten Phase, in der Grammatikinduktion und Grammatiktransduktion, bleiben die soziologische Analyse und die Interpretation von Kategorienketten jedoch nach wie vor erforderlich, da die Modelle allein die Komplexität soziologischer Hypothesen und Interpretationen nicht erfassen können.



Large Language Models (LLMs), which can imitate interactions and recognize patterns, offer interesting new possibilities for the sociological study of interaction settings. The procedure discussed here and LLMs can actually complement each other rather than replace one another. Here are some considerations on how the procedure could be expanded to benefit from LLMs:

### 1. **Interpretation and Category Formation by LLMs as Support for Grammar Induction**
   LLMs could indeed help identify category symbols for different types of sentences and conversational acts (e.g., questions, requests, responses, objections) in large corpora of transcripts. The models can imitate a wide range of interactions and understand contextualized meanings. By generating such category symbols, LLMs could act as "pre-categorizers," providing the foundation for a more in-depth structural analysis.

### 2. **Automated and Transparent Category Formation**
   One advantage of using LLMs would be the automated and flexible application of categorization logic to large datasets. Although the internal rules of LLMs remain opaque, specific fine-tuning and comparisons between human and machine categorization could enable researchers to create a reliable, empirically based foundation for categories, which can then be translated into a clear structural grammar.

### 3. **Chains of Category Symbols for Probabilistic Grammar Induction**
   The chains of category symbols generated by LLMs could then serve as input for probabilistic grammar induction. This grammar induction could model the probability and frequency of conversational sequences and reveal patterns in various interaction settings. In other words, the grammar could create a probabilistic model that compares empirical and artificially generated conversational sequences.

### 4. **Comparison of Empirical and Artificial Category Chains to Verify Models**
   Combining empirically collected category chains with artificially generated ones could serve as a validation method for LLMs. By comparing observed conversational patterns in reality with synthetically generated ones, sociological researchers could draw conclusions about the realism and limitations of LLM simulations.

### 5. **Exploring and Comparing Different Interaction Settings**
   Since LLMs can imitate specific conversational contexts, researchers could generate various artificial conversational contexts (e.g., sales conversations, advisory sessions) and then compare these with empirical data from similar settings. Using probabilistic grammar induction, differences or similarities in interaction patterns between artificial and real data could be identified and sociologically interpreted.

### 6. **Verification and Further Development of Sociological Hypotheses**
   By using artificially generated category chains to model real interactions, researchers could test and develop hypotheses about conversational structures, power dynamics, or other interaction dynamics. For instance, they could examine whether the frequency of certain interaction patterns in artificially generated contexts is similarly distributed as in reality and where significant deviations exist.

### Conclusion
This procedure is not rendered obsolete by LLMs but can instead be expanded through them. LLMs could assist in the preprocessing and automated categorization of conversational sequences. In the second phase, involving grammar induction and grammar transduction, sociological analysis and interpretation of category chains remain necessary, as the models alone cannot fully capture the complexity of sociological hypotheses and interpretations.



Es wäre möglich, Methoden wie die objektive Hermeneutik (Oevermann) und die qualitative Inhaltsanalyse (Mayring) durch den Einsatz von Deep Learning und Large Language Models (LLMs) zu ersetzen, um Lesarten, Sinnstrukturen und Kategoriensysteme effizient zu erstellen und Interaktionen zu analysieren. Deep Learning und LLMs können große Mengen an Text- und Interaktionsdaten verarbeiten und komplexe Muster und Bedeutungsstrukturen autonom erkennen, was eine automatisierte und skalierbare Alternative zur manuellen, interpretativen Analyse der objektiven Hermeneutik darstellt. Durch das Training auf umfangreichen Datensätzen sind neuronale Netzwerke in der Lage, Sinnstrukturen und Kategorien zu identifizieren, die sonst mühsam manuell extrahiert werden müssten.

Die Entwicklung einer umfassenden Handlungsgrammatik, die Sinnstrukturen und Kategorien den strukturellen Regeln sozialer Interaktionen zuordnet, bleibt jedoch ein Bereich, der spezialisierte Werkzeuge und formale Programmiersprachen wie Lisp und Scheme erfordert. Die Algorithmisch Rekursive Sequenzanalyse bietet eine regelbasierte Methode zur Induktion von Grammatiken, die die Abfolge und Struktur von Interaktionen formalisiert. Lisp und Scheme eignen sich aufgrund ihrer Fähigkeit, rekursive und regelbasierte Prozesse zu modellieren, besonders gut für die algorithmische Analyse der Sinnstrukturen. In diesem Sinne bleibt die Handlungsgrammatik ein Aufgabenbereich für spezialisierte Werkzeuge und Methoden, die tiefgehende strukturelle Einsichten in soziale Sequenzen ermöglichen.

It would be possible to replace methods such as objective hermeneutics (Oevermann) and qualitative content analysis (Mayring) with Deep Learning and Large Language Models (LLMs) to efficiently create interpretations, meaning structures, and category systems, as well as to analyze interactions. Deep Learning and LLMs can process large amounts of text and interaction data, autonomously recognizing complex patterns and meaning structures, thus providing an automated and scalable alternative to the manual interpretative analysis of objective hermeneutics. By training on extensive datasets, neural networks can identify meaning structures and categories that would otherwise have to be painstakingly extracted manually.

However, developing a comprehensive action grammar that assigns meaning structures and categories to the structural rules of social interactions remains a field requiring specialized tools and formal programming languages such as Lisp and Scheme. Algorithmic Recursive Sequence Analysis offers a rule-based method for the induction of grammars, formalizing the sequence and structure of interactions. Lisp and Scheme are particularly well-suited for the algorithmic analysis of meaning structures due to their ability to model recursive and rule-based processes. In this sense, action grammar remains a specialized area for tools and methods that enable deeper structural insights into social sequences.






In den vergangenen vier Jahrzehnten hat sich die Forschung und Datenanalyse durch technologische Fortschritte tiefgreifend gewandelt. Die algorithmisch gestützte Rekonstruktion sozialer Interaktionen erfordert heute Ansätze, die große Datenmengen und die zunehmende Komplexität sozialer Phänomene berücksichtigen. Die *Algorithmisch Rekursive Sequenzanalyse (ARS)* ist eine innovative Softwarelösung, die Forschern ermöglicht, soziale Interaktionen strukturiert zu analysieren und latente Muster in Kommunikationsabläufen sichtbar zu machen. ARS nutzt Induktion, Parsing und Transduktion, um Handlungsgrammatiken zu generieren, die latente Sinnstrukturen abbilden – Strukturen, die in sozialen Interaktionen unbewusst und wiederkehrend reproduziert werden.

Vor 40 Jahren war ein solches Werkzeug aufgrund praktischer Einschränkungen nur eingeschränkt nutzbar. Die verfügbaren Datensätze beschränkten sich auf physische Tonband- oder Videoaufnahmen (Protokolle), die zunächst in schriftliche Form (Transkripte) übertragen werden mussten. Diese Transkripte wurden dann manuell von Auswertungsteams in einzelne Interakte zerlegt und mit symbolischen Kategorien versehen – ein zeitaufwändiger Prozess, der die Analyse und das Erkennen komplexer sozialer Strukturen erschwerte. Erst danach konnte mit der Modellierung der latenten Strukturen durch Handlungsgrammatiken begonnen werden, indem Terminalzeichenketten erstellt, aus diesen durch Induktion die Grammatik abgeleitet und anschließend die Strukturen durch Parsing und Transduktion empirisch validiert wurden.

Heute hingegen kann ARS dank moderner Algorithmen, leistungsfähiger Computerinfrastruktur und umfangreicher Datensätze sein volles Potenzial entfalten. Fortschritte in der Verarbeitung natürlicher Sprache, etwa durch große Sprachmodelle, ermöglichen eine Automatisierung der Transkription, Interaktanalyse und Kategorisierung. Dadurch lassen sich die latenten Handlungsstrukturen und Sinnmuster, die sozialen Interaktionen zugrunde liegen, gezielt offenlegen. ARS ist somit nicht nur ein Werkzeug für die qualitative Sozialforschung, sondern eröffnet als interdisziplinäres Forschungsinstrument neue Möglichkeiten des Verständnisses in den Sozialwissenschaften und darüber hinaus.



Over the past four decades, research and data analysis have been profoundly transformed by technological advancements. Algorithmic reconstruction of social interactions now demands approaches that address large datasets and the increasing complexity of social phenomena. The *Algorithmic Recursive Sequence Analysis (ARS)* is an innovative software solution that enables researchers to systematically analyze social interactions and uncover latent patterns in communication sequences. ARS employs induction, parsing, and transduction to generate action grammars that capture latent semantic structures—structures that are unconsciously and recurrently reproduced within social interactions.

Forty years ago, such a tool would have been limited by practical constraints. Available datasets were confined to physical audio or video recordings (protocols), which first had to be transcribed into written form (transcripts). These transcripts were then manually segmented by research teams into individual interaction acts and assigned symbolic categories—a time-intensive process that made the analysis and identification of complex social structures difficult. Only after this process could the modeling of latent structures through action grammars begin, involving the creation of terminal symbol sequences, the induction of grammar from these sequences, and the validation of structures via parsing and transduction against empirical data.


Empirical social research is currently developing in three directions:
 
With the founding of the German Academy for Sociology, in Germany quantitative social research has regained importance and students have to learn the necessary basics again during their studies.
 
Qualitative social research continues on its way between deep hermeneutics and Oevermannian sequence analysis.
 
Multi-agent systems, neural nets, cellular automata, etc. continue to form the basis for the development of artificial social systems.
 
What is missing is qualitative social research that provides the empirically proven protocol languages for such artificial social systems, which serves to simulate empirically proven models (https://github.com/pkoopongithub). This is not possible with large language models, which hardly go beyond the explanatory value of Markov chains, but only with graph-based models that depict causal inference and rule-based action in an explanatory manner.

Die empirische Sozialforschung entwickelt sich derzeit in drei Richtungen:

Mit der Gründung der Deutschen Akademie für Soziologie hat in Deutschland die quantitative Sozialforschung wieder an Bedeutung gewonnen und die Studierenden müssen sich während des Studiums die notwendigen Grundlagen neu aneignen.
 
Die qualitative Sozialforschung bewegt sich weiter zwischen Tiefenhermeneutik und Oevermannscher Sequenzanalyse.
 
Multiagentensysteme, neuronale Netze, zellulare Automaten etc. bilden weiterhin die Grundlage für die Entwicklung künstlicher sozialer Systeme.
 
Was fehlt, ist eine qualitative Sozialforschung, die die empirisch erprobten Protokollsprachen für solche künstlichen Sozialsysteme bereitstellt, die dazu dient, empirisch erprobte Modelle zu simulieren (https://github.com/pkoopongithub). Das geht nicht mit Large Language Modellen, die kaum über den Erklärungswert von Markow-Ketten hinausgehen, sondern nur mit graphenbasierten Modellen, die kausale Inferenz und regelbasiertes Handeln erklärend abbilden.





Es wäre möglich, Methoden wie die objektive Hermeneutik (Oevermann) und die qualitative Inhaltsanalyse (Mayring) durch den Einsatz von Deep Learning und Large Language Models (LLMs) zu ersetzen, um Lesarten, Sinnstrukturen und Kategoriensysteme effizient zu erstellen und Interaktionen zu analysieren. Deep Learning und LLMs können große Mengen an Text- und Interaktionsdaten verarbeiten und komplexe Muster und Bedeutungsstrukturen autonom erkennen, was eine automatisierte und skalierbare Alternative zur manuellen, interpretativen Analyse der objektiven Hermeneutik darstellt. Durch das Training auf umfangreichen Datensätzen sind neuronale Netzwerke in der Lage, Sinnstrukturen und Kategorien zu identifizieren, die sonst mühsam manuell extrahiert werden müssten.

Die Entwicklung einer umfassenden Handlungsgrammatik, die Sinnstrukturen und Kategorien den strukturellen Regeln sozialer Interaktionen zuordnet, bleibt jedoch ein Bereich, der spezialisierte Werkzeuge und formale Programmiersprachen wie Lisp und Scheme erfordert. Die Algorithmisch Rekursive Sequenzanalyse bietet eine regelbasierte Methode zur Induktion von Grammatiken, die die Abfolge und Struktur von Interaktionen formalisiert. Lisp und Scheme eignen sich aufgrund ihrer Fähigkeit, rekursive und regelbasierte Prozesse zu modellieren, besonders gut für die algorithmische Analyse der Sinnstrukturen. In diesem Sinne bleibt die Handlungsgrammatik ein Aufgabenbereich für spezialisierte Werkzeuge und Methoden, die tiefgehende strukturelle Einsichten in soziale Sequenzen ermöglichen.



It would be possible to replace methods such as objective hermeneutics (Oevermann) and qualitative content analysis (Mayring) with Deep Learning and Large Language Models (LLMs) to efficiently create interpretations, meaning structures, and category systems, as well as to analyze interactions. Deep Learning and LLMs can process large amounts of text and interaction data, autonomously recognizing complex patterns and meaning structures, thus providing an automated and scalable alternative to the manual interpretative analysis of objective hermeneutics. By training on extensive datasets, neural networks can identify meaning structures and categories that would otherwise have to be painstakingly extracted manually.

However, developing a comprehensive action grammar that assigns meaning structures and categories to the structural rules of social interactions remains a field requiring specialized tools and formal programming languages such as Lisp and Scheme. Algorithmic Recursive Sequence Analysis offers a rule-based method for the induction of grammars, formalizing the sequence and structure of interactions. Lisp and Scheme are particularly well-suited for the algorithmic analysis of meaning structures due to their ability to model recursive and rule-based processes. In this sense, action grammar remains a specialized area for tools and methods that enable deeper structural insights into social sequences.


Wenn das Ziel darin besteht, Verkaufsgespräche zu erklären – also die zugrunde liegenden Regeln, Ziele und Strukturen offen darzustellen –, dann ist ein generatives Modell wie ein LLM oft unzureichend. Ein LLM kann lediglich ähnliche Gespräche erzeugen oder imitieren, ohne die Absichten oder sozialen Mechanismen dahinter explizit zu machen.

 Wesentliche Punkte des Arguments

1. Nachahmung vs. Erklärung: LLMs sind darauf trainiert, Muster aus großen Textmengen zu lernen und Antworten zu generieren, die oberflächlich betrachtet einem Verkaufsgespräch ähneln können. Aber sie haben keine explizite Darstellung der Ziele, Strategien oder Regeln, die Verkaufsgespräche formen. Somit bleibt die Erklärung der zugrunde liegenden Strukturen unzureichend.
  
2. Erklärungsdefizit: Verkaufsgespräche folgen oft bestimmten Regeln und Zielen, wie zum Beispiel dem Aufbau von Vertrauen, der Ermittlung von Kundenbedürfnissen oder der Anwendung spezifischer Verhandlungstechniken. Ein LLM kann zwar solche Techniken reproduzieren, erklärt jedoch nicht, warum und wie diese Strategien funktionieren. Hierfür wäre ein analytischer Ansatz notwendig, der die Struktur und Dynamik solcher Gespräche explizit modelliert.

3. Beitrag von LLMs zu Verständniszwecken begrenzt: Da LLMs auf Wahrscheinlichkeitsverteilungen und Muster basieren, zeigen sie keine bewusste Zielgerichtetheit und keine kausalen Regeln, die Verkaufsgespräche leiten. Wenn das Ziel also darin besteht, die Funktionsweise dieser Gespräche zu verstehen und zu lehren, wäre ein LLM allein unzureichend – es braucht ergänzende Modelle oder Theorien, die die Handlungslogiken und Ziele explizit beschreiben.

 Ergänzende Ansätze zur Erklärung von Verkaufsgesprächen

- Regelbasierte Modelle: Durch die Verwendung von regelbasierten Systemen oder graphenbasierten Modellen könnten die Handlungslogiken, Ziele und Entscheidungsstrukturen von Verkaufsgesprächen explizit dargestellt werden. Diese Modelle könnten die spezifischen Verhaltensweisen und Reaktionen in einem Verkaufsgespräch verdeutlichen und die Verbindungen zwischen verschiedenen Gesprächszielen und -taktiken erklären.
  
- Agentenbasierte Modelle: Multiagentensysteme, die auf kausalen und regelbasierten Prinzipien basieren, könnten die Dynamik und das Ziel eines Verkaufsgesprächs explizit abbilden. Solche Systeme können zudem eine gewisse Zielgerichtetheit simulieren, die Verkaufsstrategien realistischer und transparenter macht.

- Dialoggrammatiken: Mit spezifischen Protokollen oder Dialoggrammatiken könnten die Abläufe und Regeln für Verkaufsgespräche detaillierter dargestellt und somit besser erklärt werden, als es durch ein generatives Modell möglich ist.




Das Argument ist schlüssig, da es darauf hinweist, dass die Fähigkeit, Verkaufsgespräche zu imitieren, nicht mit der Fähigkeit gleichzusetzen ist, diese Gespräche in ihren Regeln und Zielen zu erklären. Ein LLM mag Verkaufsgespräche simulieren können, aber ohne ein explizites Regelwerk und ohne Verständnis für die Ziele solcher Gespräche liefert es keine adäquate Erklärung.


Die Algorithmisch Rekursive Sequenzanalyse ist eine Methode zur Kausalinferenz mit Handlungsgrammatiken und Graphen. Im Gegensatz zu Poststrukturalisten, Postmodernisten, kritischen Posthumanisten, Konstruktivisten und Tiefenhermeneuten nimmt sie Karl Popper, Ulrich Oevermann, Chomsky, Pearl, Bayes, LISP, SCHEME, R und Python sehr ernst.

Algorithmic Recursive Sequence Analysis is a method for causal inference using action grammars and graphs. In contrast to poststructuralists, postmodernists, critical posthumanists, constructivists and depth hermeneutics, she takes Karl Popper, Ulrich Oevermann, Chomsky, Pearl, Bayes, LISP, SCHEME, R and Python very seriously.

If the goal is to explain sales conversations – that is, to openly present the underlying rules, goals and structures – then a generative model like an LLM is often inadequate. An LLM can only create or imitate similar conversations without making explicit the intentions or social mechanisms behind them.

 Key points of the argument

1. Imitation vs. Explanation: LLMs are trained to learn patterns from large amounts of text and generate responses that, on the surface, can resemble a sales pitch. But they have no explicit representation of the goals, strategies, or rules that shape sales conversations. The explanation of the underlying structures therefore remains inadequate.
  
2. Explanation deficit: Sales conversations often follow certain rules and goals, such as building trust, identifying customer needs or using specific negotiation techniques. While an LLM can reproduce such techniques, it does not explain why and how these strategies work. This would require an analytical approach that explicitly models the structure and dynamics of such conversations.

3. Contribution of LLMs to understanding purposes limited: Because LLMs are based on probability distributions and patterns, they do not demonstrate conscious targeting and causal rules guiding sales conversations. So if the goal is to understand and teach how these conversations work, an LLM alone would be inadequate - it needs complementary models or theories that explicitly describe the logic of action and goals.

 Complementary approaches to explaining sales conversations

- Rule-based models: By using rule-based systems or graph-based models, the action logic, goals and decision structures of sales discussions could be explicitly represented. These models could clarify the specific behaviors and reactions in a sales conversation and explain the connections between different conversation goals and tactics.
  
- Agent-based models: Multi-agent systems based on causal and rule-based principles could explicitly represent the dynamics and goal of a sales conversation. Such systems can also simulate a certain level of targeting that makes sales strategies more realistic and transparent.

- Dialogue grammars: With specific protocols or dialogue grammars, the processes and rules for sales discussions could be presented in more detail and thus better explained than is possible with a generative model.


Projektziele
Erklärung sozialer Interaktionen: Anstatt komplexe soziale Interaktionen und Verkaufsgespräche durch Black-Box-Modelle abzubilden, nutzen wir eine regelbasierte Sprache, die nachvollziehbare und interpretierbare Analysen ermöglicht.
Einsatz in Multiagentensystemen: Die entwickelte Sprache kann als Protokoll für Multiagentensysteme dienen und soll in Python implementiert werden. Der Einsatz solcher Protokollsprachen in Multiagentensystemen eröffnet neue Möglichkeiten, transparente und erklärbare Interaktionen zwischen Agenten zu ermöglichen.
Innovative Kombination von Methodologien: Der Ansatz verbindet Methoden der soziologischen Hermeneutik und Inhaltsanalyse mit einer formalisierten Sprachentwicklung. Hierdurch wird eine modellbasierte Grammatik induziert, die verschiedene Terminal- und Nicht-Terminal-Symbole umfasst, welche soziale Interaktionen als erklärbare Sequenzen abbilden.

Project Goals
Explaining Social Interactions: Instead of modeling complex social interactions and sales dialogues through black-box models, this project uses a rule-based language that enables understandable and interpretable analyses.
Use in Multi-Agent Systems: The developed language can serve as a protocol for multi-agent systems and will be implemented in Python. The use of such protocol languages in multi-agent systems opens up new possibilities for transparent and explainable interactions among agents.
Innovative Methodological Combination: This approach merges sociological hermeneutics and content analysis with formalized language development. A model-based grammar is induced, encompassing various terminal and non-terminal symbols that represent social interactions as explainable sequences.


Generative Pre-Trained Transformers (GPT) und Large Language Models (LLM) gehen kaum über den Erklärungswert von Markow-Ketten hinaus und müssen zudem mit der Wissensbasis empirisch ermittelter Dialoggrammatiken (Algorithmisch rekursive Sequenzanalyse) und agentenorientierter gewichteter Entscheidungstabellen für eine bessere Ergebnisqualität optimiert werden. Nur so werden Diaogschnittstellen glaubwürdiger als Markow-Generatoren und nur so werden Protokollsprachen für Agenten empirisch bewährte Dialogstrukturen abbilden.

Generative Pre-Trained Transformers (GPT) and Large Language Models (LLM) hardly go beyond the explanatory value of Markov chains and must also be optimized with the knowledge base of empirically determined dialog grammars (algorithmically recursive sequence analysis) and agent-oriented weighted decision tables for better quality results. Only in this way will dialog interfaces become more credible than Markov generators and only in this way will protocol languages for agents map empirically proven dialog structures.
